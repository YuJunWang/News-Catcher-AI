{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tDWf2hbmDM1t",
        "a6-SvUdz-ECS",
        "-G4IJVr-DcLA",
        "BGmOmxDpD3aY",
        "bzFcVunVEWOJ",
        "Xey2E3KuEjaU",
        "92AfOrPyE1QG",
        "tL9KMrhtotzw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# å®‰è£å¿…è¦å¥—ä»¶\\æ›è¼‰Google drive"
      ],
      "metadata": {
        "id": "tDWf2hbmDM1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sD18Eoe9C4UK"
      },
      "outputs": [],
      "source": [
        "!pip install -U requests beautifulsoup4 pandas openpyxl matplotlib wordcloud google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import drive, userdata\n",
        "from datetime import datetime, timedelta, timezone"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n--otJcamtBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æœå°‹ä»¥åŠåŠŸèƒ½è¨­å®š"
      ],
      "metadata": {
        "id": "9fJzkeismKgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= CONFIGURATION START =================\n",
        "\n",
        "# 1. Google Gemini API Key\n",
        "\"\"\"\n",
        "è«‹åˆ° https://aistudio.google.com/ ç”³è«‹ä¸€çµ„free tieræˆ–ä»¥ä¸Šçš„APIå¯†ç¢¼ï¼Œ\n",
        "\n",
        "å¯ä»¥ä½¿ç”¨colabçš„Secrets(å·¦å´é‘°åŒ™åœ–æ¨£)å°‡APIå„²å­˜èµ·ä¾†ï¼Œä¸¦å–åç‚ºGOOGLE_API_KEYï¼Œè¨­å®šçš„ç¨‹å¼ç¢¼æœƒè‡ªå‹•å¼•å…¥ç›´æ¥ä½¿ç”¨ï¼›\n",
        "\n",
        "æˆ–æ˜¯åœ¨ä¸‹æ–¹GOOGLE_API_KEY = \"è«‹åœ¨æ­¤å¡«å…¥Keyä½œç‚ºå‚™ç”¨\"ï¼Œç›´æ¥å¡«å…¥API KEY (ä¸å»ºè­°é€™æ¨£åš) ã€‚\n",
        "\n",
        "åˆ‡è¨˜ï¼Œè«‹å‹¿å¤–æµæ‚¨çš„API KEYã€‚\n",
        "\"\"\"\n",
        "try:\n",
        "    # å˜—è©¦å¾ Colab Secrets è®€å–\n",
        "    GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"ğŸ”‘ å·²å¾ Secrets å®‰å…¨è®€å– API Key\")\n",
        "except Exception as e:\n",
        "    # å¦‚æœè®€å–å¤±æ•— (ä¾‹å¦‚æ²’è¨­å®š Secrets)ï¼Œæé†’ä½¿ç”¨è€…\n",
        "    print(\"âš ï¸ æœªåµæ¸¬åˆ° Secretsï¼Œè«‹ç¢ºèªæ˜¯å¦å·²åœ¨å·¦å´ 'é‘°åŒ™' åœ–ç¤ºä¸­è¨­å®š 'GOOGLE_API_KEY'\")\n",
        "    GEMINI_API_KEY = \"è«‹åœ¨æ­¤å¡«å…¥Keyä½œç‚ºå‚™ç”¨\"\n",
        "\n",
        "# 2. ç›£æ¸¬ä¸»é¡Œ\n",
        "MONITOR_CONFIG = {\n",
        "    # 1. ç§‘æŠ€å·¨é ­ï¼šå±•ç¤º AI æ“´å±•èƒ½å¦æŠ“åˆ°å…·é«”ç”¢å“ (å¦‚ Blackwell, CoWoS)\n",
        "    \"ç§‘æŠ€è¶¨å‹¢\": [\"è¼é” NVIDIA\", \"OpenAI\", \"é´»æµ·\"],\n",
        "\n",
        "    # 2. æ¶ˆè²»å“ç‰Œï¼šé€™æ˜¯å±•ç¤ºã€Œèªæ„éæ¿¾ã€çš„æœ€ä½³æˆ°å ´\n",
        "    #    (æ˜Ÿå·´å…‹/å¥½å¸‚å¤šå¸¸æœ‰ã€Œå¥§å®¢äº‹ä»¶ã€æˆ–ã€Œç¤¾æœƒæ¡ˆä»¶ã€ï¼Œçœ‹ AI èƒ½å¦å‰”é™¤ï¼Œåªç•™ç‡Ÿé‹/ä¿ƒéŠ·)\n",
        "    \"å“ç‰Œå‹•æ…‹\": [\"æ˜Ÿå·´å…‹\", \"å¥½å¸‚å¤š Costco\", \"ç‰¹æ–¯æ‹‰ Tesla\"],\n",
        "\n",
        "    # 3. é«˜æ³¢å‹•è³‡ç”¢ï¼šå±•ç¤º AI è­˜åˆ¥ã€Œè©é¨™ã€èˆ‡ã€Œè²¡ç¶“åˆ†æã€çš„èƒ½åŠ›\n",
        "    #    (æ¯”ç‰¹å¹£å¸¸æ··é›œè©é¨™æ–°èï¼›é»ƒé‡‘å¸¸æ··é›œéŠ€æ¨“æ¶æ¡ˆ)\n",
        "    \"é‡‘èå¸‚å ´\": [\"æ¯”ç‰¹å¹£\", \"é»ƒé‡‘åƒ¹æ ¼\", \"00940\"]\n",
        "}\n",
        "\n",
        "# 3. çˆ¬èŸ²è¨­å®š\n",
        "# æ–°èæ”¶é›†å›æ¨æ™‚é–“\n",
        "LOOKBACK_DAYS = 3\n",
        "# æ¯å€‹é—œéµå­—æŸ¥æ‰¾ç­†æ•¸ä¸Šé™\n",
        "MAX_ITEMS_PER_KEYWORD = 50\n",
        "\n",
        "# 4. AI è¨­å®š\n",
        "ENABLE_KEYWORD_EXPANSION = True\n",
        "ENABLE_AI_SUMMARY = True\n",
        "ENABLE_SENTIMENT = True\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# 5. è¼¸å‡ºè¨­å®š\n",
        "# ä½¿ç”¨æ­¤è¨­å®šåœ¨Google Driveä¸­å»ºç«‹\"News_Crawler_Reports\"è³‡æ–™å¤¾å­˜æ”¾çµæœå ±å‘Š\n",
        "OUTPUT_FOLDER = \"/content/drive/My Drive/News_Crawler_Reports\"\n",
        "\n",
        "# ================= CONFIGURATION END ================="
      ],
      "metadata": {
        "id": "56-z3yZOVYo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ç¨‹å¼ç¢¼å€å¡Š"
      ],
      "metadata": {
        "id": "a6-SvUdz-ECS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡çµ„ A: å…¨å±€é…ç½® (Configuration)"
      ],
      "metadata": {
        "id": "-G4IJVr-DcLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆå§‹åŒ– Gemini\n",
        "if ENABLE_AI_SUMMARY and GOOGLE_API_KEY and GOOGLE_API_KEY != \"è«‹åœ¨æ­¤å¡«å…¥Keyä½œç‚ºå‚™ç”¨\":\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # æŒ‡å®šæ¨¡å‹\n",
        "    target_model = 'gemini-2.5-flash-lite'\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(target_model)\n",
        "        print(f\"âœ… å·²æˆåŠŸé€£æ¥æ¨¡å‹: {target_model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ é€£æ¥ {target_model} å¤±æ•—: {e}\")\n",
        "        print(\"ğŸ”„ å˜—è©¦åˆ‡æ›è‡³å‚™æ´æ¨¡å‹: gemini-2.0-flash\")\n",
        "        model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "else:\n",
        "    print(\"âš ï¸ æœªè¨­å®š API Keyï¼ŒAI åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚\")\n",
        "\n",
        "# æ›è¼‰ Google Drive\n",
        "def setup_drive():\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    tz_tw = timezone(timedelta(hours=8))\n",
        "    today_str = datetime.now(tz_tw).strftime(\"%Y-%m-%d\")\n",
        "    target_path = os.path.join(OUTPUT_FOLDER, today_str)\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "    print(f\"âœ… é›²ç«¯å„²å­˜è·¯å¾‘å·²ç¢ºèª: {target_path}\")\n",
        "    return target_path\n",
        "\n",
        "OUTPUT_PATH = setup_drive()"
      ],
      "metadata": {
        "id": "XRgWO7j-om3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "def expand_keywords_structured(original_config):\n",
        "    \"\"\"\n",
        "    AI é—œéµå­—æ“´å±•\n",
        "    ç­–ç•¥ï¼šæƒ…å¢ƒè‡ªé©æ‡‰ (Context-Adaptive)\n",
        "    ç‰¹é»ï¼šä¸é™å®šé ˜åŸŸï¼Œæ ¹æ“šã€Œä¸»é¡Œã€è‡ªå‹•åˆ¤æ–·è©²æ‰¾å…¬å¸ã€æ‰¾æ³•æ¡ˆã€é‚„æ˜¯æ‰¾ç†±é–€å•†å“ã€‚\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§  æ­£åœ¨å•Ÿå‹• AI é—œéµå­—æ“´å±•å¼•æ“...\")\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_year_str = now.strftime(\"%Y\")       # e.g., \"2025\"\n",
        "    current_month_str = now.strftime(\"%Yå¹´%mæœˆ\") # e.g., \"2025å¹´11æœˆ\"\n",
        "\n",
        "    structured_config = {}\n",
        "\n",
        "    for topic, keywords in original_config.items():\n",
        "        structured_config[topic] = {}\n",
        "\n",
        "        # å–å¾—è©²ä¸»é¡Œä¸‹çš„æ‰€æœ‰æºé ­é—œéµå­—ï¼Œä¾› AI åƒè€ƒä»¥é¿å…æ’è»Š\n",
        "        siblings = \", \".join(keywords)\n",
        "\n",
        "        for base_kw in keywords:\n",
        "            print(f\"   ğŸŒ¿ æºé ­é—œéµå­— [{base_kw}] ({topic}) åˆ†æä¸­...\")\n",
        "\n",
        "            # --- Prompt é€šç”¨åŒ–å‡ç´š ---\n",
        "            prompt = f\"\"\"\n",
        "            ä½ æ˜¯ä¸€åç²¾é€šã€Œå°ç£æ–°èç†±é»ã€èˆ‡äº†è§£ã€Œé—œéµå­—ç›¸é—œæ€§ã€çš„è³‡æ·±æ–°èç·¨è¼¯ã€‚\n",
        "            **ç¾åœ¨çš„æ™‚é–“æ˜¯ï¼š{current_month_str}ã€‚**\n",
        "            æˆ‘çš„ç›£æ¸¬ä¸»é¡Œæ˜¯ã€Œ{topic}ã€ï¼Œç›®å‰è¨­å®šçš„æºé ­é—œéµå­—æ¸…å–®åŒ…å«ï¼š[{siblings}]ã€‚\n",
        "            è«‹é‡å°ã€Œ{base_kw}ã€é€™å€‹é—œéµå­—ï¼Œå…ˆé‡å°é€™å€‹æ™‚é–“é»å…ˆåˆæ­¥ç ”ç©¶æä¾›çµ¦ä½ çš„é—œéµå­—ã€‚\n",
        "            å†æä¾› 2 å€‹ã€Œ**{current_month_str}ç•¶ä¸‹æœ€å…·æ–°èåƒ¹å€¼ã€ä¸”ã€Œå…·å‚™æœå°‹å€åˆ¥åº¦ã€çš„ç¹é«”ä¸­æ–‡æ–°èæœå°‹é—œéµå­—ã€‚\n",
        "            **ç›¡é‡æä¾›ç¬¦åˆä¸»é¡Œæ¡†æ¶ä¸”å…·å‚™å»¶ä¼¸æ€§çš„é—œéµå­—**\n",
        "\n",
        "            ã€æ€è€ƒç­–ç•¥ï¼šè«‹æ ¹æ“šã€Œ{topic}ã€çš„æ€§è³ªèª¿æ•´ç­–ç•¥ã€‘\n",
        "            1. è‹¥æ˜¯**ç”¢æ¥­/ç§‘æŠ€/è²¡ç¶“**ï¼šè«‹æ‰¾å‡ºã€Œé¾é ­ä¼æ¥­ã€ã€ã€Œæ ¸å¿ƒæŠ€è¡“ã€ã€ã€Œå…·é«”ç”¢å“å‹è™Ÿã€æˆ–ã€Œé—œéµæ•¸æ“šæŒ‡æ¨™ã€ã€‚\n",
        "            2. è‹¥æ˜¯**ç¤¾æœƒ/æ”¿ç­–/ç’°ä¿**ï¼šè«‹æ‰¾å‡ºã€Œæ³•æ¡ˆåç¨±ã€ã€ã€Œå°ˆæœ‰ç¾è±¡åè©ã€(å¦‚:è¡Œäººåœ°ç„)ã€ã€Œæ ¸å¿ƒäººç‰©ã€æˆ–ã€Œçˆ­è­°äº‹ä»¶ã€ã€‚\n",
        "            3. è‹¥æ˜¯**ç”Ÿæ´»/å“ç‰Œ/æ¶ˆè²»**ï¼šè«‹æ‰¾å‡ºã€Œç†±é–€å“é …ã€ã€ã€Œä¿ƒéŠ·æ´»å‹•åç¨±ã€ã€ã€Œç«¶çˆ­å°æ‰‹ã€æˆ–ã€Œé—œè¯å¹³å°ã€ã€‚\n",
        "\n",
        "            ã€æ™‚æ•ˆæ€§è¦å‰‡ã€‘\n",
        "            1. âŒ **ç¦æ­¢æ­·å²èˆŠè**ï¼šçµ•å°ä¸è¦æä¾›ä¸æ˜¯æ–°èçš„éæœŸçˆ­è­°ã€‚\n",
        "            2. âœ… **é–å®šè¿‘æœŸå‹•æ…‹**ï¼š\n",
        "               - è‹¥æ˜¯æ”¿æ²»/å¤–äº¤ï¼Œæœæœ€è¿‘ 3 å€‹æœˆçš„è«‡è©±æˆ–äº‹ä»¶ï¼ˆå¦‚ï¼šæœ€æ–°è»å”®é€²åº¦ã€ä¸–ç•Œé‡è¦æœƒè­°ï¼‰ã€‚\n",
        "               - è‹¥æ˜¯è²¡ç¶“ï¼Œæœç•¶å­£çš„æ•¸æ“šæˆ–è¶¨å‹¢ã€‚\n",
        "            3. âœ… **åŠ ä¸Šå¹´ä»½/æ™‚é–“è©**ï¼šè‹¥è©²è­°é¡Œå…·é€±æœŸæ€§ï¼Œè«‹åŠ ä¸Šã€Œ{current_month_str}ã€æˆ–ã€Œæœ€æ–°ã€ã€‚\n",
        "\n",
        "            ã€åš´æ ¼çš„æ€è€ƒè·¯å¾‘ã€‘\n",
        "            1. âŒ **çµ•å°ç¦æ­¢åè©è§£é‡‹**ï¼šè‹¥æºé ­æ˜¯ã€ŒCPIã€ï¼Œç¦æ­¢å›å‚³ã€Œæ¶ˆè²»è€…ç‰©åƒ¹æŒ‡æ•¸ã€ï¼ˆé€™æ˜¯ç„¡æ•ˆæ“´å±•ï¼‰ã€‚\n",
        "            2. âŒ **çµ•å°ç¦æ­¢å–®ç´”åŒç¾©è©**ï¼šè‹¥æºé ­æ˜¯ã€Œå°è‚¡ã€ï¼Œç¦æ­¢å›å‚³ã€Œå°ç£è‚¡å¸‚ã€ã€‚\n",
        "            3. âœ… **å°‹æ‰¾ã€Œé©…å‹•å› å­ã€**ï¼š\n",
        "               - ç”¢å“é¡ï¼šåŠ ä¸Š**ä¸»æ‰“åŠŸèƒ½**æˆ–**ç”¢å“ç‰¹è‰²**ï¼ˆå¦‚ï¼šiPhone 17 pro -> **ç›¸æ©Ÿ**ï¼‰ã€‚\n",
        "               - é‡‘èé¡ï¼šåŠ ä¸Šè¶¨å‹¢å‹•è©ï¼ˆå¦‚ï¼šå°å¹£ -> **é‡è²¶**ã€å¤®è¡Œ -> **å¹²é **ï¼‰ã€‚\n",
        "               - ç”¢æ¥­é¡ï¼šæ‰¾å‡º**é ˜æ¼²æ—ç¾¤**æˆ–**ç‰¹å®šæŠ€è¡“**ï¼ˆå¦‚ï¼šå°è‚¡ -> **AIä¼ºæœå™¨**ã€**é‡é›»è‚¡**ï¼‰ã€‚\n",
        "               - æ”¿ç­–é¡ï¼šæ‰¾å‡º**å…·é«”æ³•æ¡ˆ**æˆ–**çˆ­è­°é»**ï¼ˆå¦‚ï¼šæˆ¿å¸‚æ”¿ç­– -> **æ–°é’å®‰**ã€**å›¤æˆ¿ç¨…2.0**ï¼‰ã€‚\n",
        "\n",
        "            ã€è¼¸å‡ºç¯„ä¾‹åƒè€ƒã€‘\n",
        "            å‹™å¿…è¨˜å¾—åƒè€ƒæŸ¥è©¢ç•¶ä¸‹æˆ–æ—¥æœŸå€é–“å…§çš„ç¤¾æœƒè¶¨å‹¢ï¼Œä½†ç„¡é ˆéåº¦çµ„åˆã€å»¶ä¼¸é—œéµå­—\n",
        "            - (é‡‘è) è¼¸å…¥: ç¾åœ‹è¯æº–æœƒ -> è¼¸å‡º: [\"é®‘çˆ¾ è«‡è©±\", \"é™æ¯ æ™‚é–“è¡¨\"] (æ•æ‰å¸‚å ´é æœŸ)\n",
        "            - (ç”¢æ¥­) è¼¸å…¥: åŠå°é«” -> è¼¸å‡º: [\"CoWoS ç”¢èƒ½\", \"å…ˆé€²å°è£\"] (æ•æ‰æŠ€è¡“ç“¶é ¸)\n",
        "            - (æ”¿ç­–) è¼¸å…¥: æˆ¿å¸‚æ”¿ç­– -> è¼¸å‡º: [\"æ–°é’å®‰ é•è¦\", \"å¤®è¡Œ é™è²¸ä»¤\"] (æ•æ‰ç†±é–€è­°é¡Œ)\n",
        "            - (ç¤¾æœƒ) è¼¸å…¥: å°‘å­åŒ– -> è¼¸å‡º: [\"äººå·¥ç”Ÿæ®–æ³•\", \"é¾å¹´ ç”Ÿè‚²ç‡\"] (æ•æ‰æ³•è¦èˆ‡æ•¸æ“š)\n",
        "            - (æ¶ˆè²») è¼¸å…¥: å‡ºåœ‹æ—…éŠ -> è¼¸å‡º: [\"æ©Ÿç¥¨ ä¿ƒéŠ·\", \"æ—¥å¹£ åŒ¯ç‡\"] (æ•æ‰æˆæœ¬èª˜å› )\n",
        "\n",
        "\n",
        "            è«‹å›å‚³ JSON String List(åªå–æœ€å¥½çš„ 2 å€‹)ï¼Œç¯„ä¾‹ï¼š[\"é—œéµå­—A\", \"é—œéµå­—B\"]\n",
        "            \"\"\"\n",
        "\n",
        "            search_terms = [base_kw] # é è¨­åŒ…å«è‡ªå·±\n",
        "\n",
        "            try:\n",
        "                # å‘¼å« Gemini\n",
        "                response = model.generate_content(prompt)\n",
        "                text = response.text.strip()\n",
        "\n",
        "                # æ¸…ç† Markdown æ¨™è¨˜\n",
        "                if \"```json\" in text:\n",
        "                    text = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "                elif \"```\" in text:\n",
        "                    text = text.split(\"```\")[1].strip()\n",
        "\n",
        "                ai_suggestions = json.loads(text)\n",
        "\n",
        "                # éæ¿¾èˆ‡åŠ å…¥\n",
        "                for term in ai_suggestions:\n",
        "                    if term == base_kw: continue\n",
        "                    # ç°¡å–®éæ¿¾ï¼šè‹¥å»ºè­°è©å¤ªçŸ­æˆ–æ˜¯å–®ç´”çš„åŒ…å«é—œä¿‚ï¼Œå¯èƒ½æ•ˆç›Šä¸é«˜\n",
        "                    if len(term) > len(base_kw) and base_kw in term and len(term) - len(base_kw) < 2:\n",
        "                        continue\n",
        "                    search_terms.append(term)\n",
        "\n",
        "                # å»é‡\n",
        "                structured_config[topic][base_kw] = list(set(search_terms))\n",
        "\n",
        "                print(f\"      â””â”€â”€ ğŸƒ æ·±åº¦é—œéµå­—: {structured_config[topic][base_kw]}\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      âš ï¸ æ“´å±•å¤±æ•— ({e})ï¼Œåƒ…ä½¿ç”¨åŸå§‹é—œéµå­—ã€‚\")\n",
        "                structured_config[topic][base_kw] = [base_kw]\n",
        "\n",
        "    return structured_config"
      ],
      "metadata": {
        "id": "ZkRVXpJ_X_g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡çµ„ B: åŸºç¤å·¥å…·èˆ‡ç¶²è·¯è«‹æ±‚ (Utils & Network)"
      ],
      "metadata": {
        "id": "BGmOmxDpD3aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import urllib.parse\n",
        "from datetime import datetime, timedelta\n",
        "from email.utils import parsedate_to_datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ==========================================\n",
        "# åŸºç¤å·¥å…·ï¼šè«‹æ±‚æ ¸å¿ƒ\n",
        "# ==========================================\n",
        "\n",
        "try:\n",
        "    LIMIT = MAX_ITEMS_PER_KEYWORD\n",
        "except NameError:\n",
        "    LIMIT = 10 # é è¨­å‚™æ´\n",
        "\n",
        "try:\n",
        "    # è®“çˆ¬èŸ²çŸ¥é“æ™‚é–“ç•Œç·šï¼Œç”¨æ–¼å³æ™‚éæ¿¾\n",
        "    DAYS_LIMIT = LOOKBACK_DAYS\n",
        "except NameError:\n",
        "    DAYS_LIMIT = 2\n",
        "\n",
        "TW_TZ = timezone(timedelta(hours=8))\n",
        "\n",
        "def get_now_tw():\n",
        "    \"\"\"å–å¾—å°ç£ç•¶ä¸‹çš„ Naive Time (ä¸å¸¶æ™‚å€æ¨™ç±¤ï¼Œæ–¹ä¾¿è¨ˆç®—)\"\"\"\n",
        "    return datetime.now(TW_TZ).replace(tzinfo=None)\n",
        "\n",
        "def get_random_header():\n",
        "    \"\"\"\n",
        "    éš¨æ©Ÿç”¢ç”Ÿ User-Agentï¼ŒåŒ…å«é›»è…¦ç‰ˆèˆ‡æ‰‹æ©Ÿç‰ˆï¼Œ\n",
        "    è®“çˆ¬èŸ²çœ‹èµ·ä¾†åƒä¾†è‡ªä¸åŒè£ç½®çš„æµé‡ã€‚\n",
        "    \"\"\"\n",
        "    user_agents = [\n",
        "        # Chrome Windows\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
        "        # Chrome Mac\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15\",\n",
        "        # Firefox Windows\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "        # Edge\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0\"\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(user_agents),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "        \"Referer\": \"https://www.google.com/\"\n",
        "    }\n",
        "\n",
        "def fetch_page_robust(url, source_name=\"Unknown\", max_retries=3):\n",
        "    \"\"\"\n",
        "    å …éŸŒè«‹æ±‚å‡½å¼ (å« 404/410 æ”¾æ£„é‚è¼¯)\n",
        "    æ”¯æ´ RSS XML è§£æ\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            sleep_time = random.uniform(1.0, 2.0) + (attempt * 1.2) # ç¿»é æ™‚ç¨å¾®ç¸®çŸ­å»¶é²\n",
        "            time.sleep(sleep_time)\n",
        "            headers = get_random_header()\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "            if response.status_code in [404, 410]:\n",
        "                return None # è³‡æºä¸å­˜åœ¨ï¼Œç›´æ¥æ”¾æ£„\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                # å¦‚æœæ˜¯ RSSï¼Œä½¿ç”¨ xml è§£æå™¨\n",
        "                if \"rss\" in url or \"xml\" in response.headers.get('Content-Type', ''):\n",
        "                    return BeautifulSoup(response.content, 'xml')\n",
        "                else:\n",
        "                    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # print(f\"   âš ï¸ [{source_name}] HTTP {response.status_code} (Retry {attempt+1})...\")\n",
        "        except Exception as e:\n",
        "            # print(f\"   âš ï¸ [{source_name}] Err: {e} (Retry {attempt+1})...\")\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "def parse_relative_time(time_str):\n",
        "    \"\"\"æ™‚é–“æ ¼å¼è™•ç†\"\"\"\n",
        "    now = get_now_tw()\n",
        "    time_str = str(time_str).strip()\n",
        "    try:\n",
        "        if \"å°æ™‚\" in time_str or \"hour\" in time_str:\n",
        "            hours = int(''.join(filter(str.isdigit, time_str)))\n",
        "            return now - timedelta(hours=hours)\n",
        "        elif \"åˆ†\" in time_str or \"min\" in time_str:\n",
        "            mins = int(''.join(filter(str.isdigit, time_str)))\n",
        "            return now - timedelta(minutes=mins)\n",
        "        elif \"å¤©\" in time_str or \"day\" in time_str or \"æ˜¨\" in time_str:\n",
        "            days = 1\n",
        "            if any(char.isdigit() for char in time_str):\n",
        "                days = int(''.join(filter(str.isdigit, time_str)))\n",
        "            return now - timedelta(days=days)\n",
        "        else:\n",
        "            for fmt in (\"%Y-%m-%d %H:%M\", \"%Y/%m/%d %H:%M\", \"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y.%m.%d\"):\n",
        "                try:\n",
        "                    return datetime.strptime(time_str, fmt)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "            return now\n",
        "    except:\n",
        "        return now\n",
        "\n",
        "def fetch_article_content(url, source):\n",
        "    soup = fetch_page_robust(url, f\"FullText_{source}\")\n",
        "    if not soup: return None, None\n",
        "\n",
        "    content = \"\"\n",
        "    actual_date = None\n",
        "\n",
        "    try:\n",
        "        # --- 1. é‡å° Yahoo çš„å¼·åŠ›æ™‚é–“è§£æ (JSON-LD å„ªå…ˆ) ---\n",
        "        if source == \"Yahooæ–°è\":\n",
        "            container = soup.select_one(\".caas-body\")\n",
        "\n",
        "            def convert_to_tw_time(iso_str):\n",
        "                try:\n",
        "                    # å…ˆè®€æˆ Aware UTC\n",
        "                    dt_utc = datetime.fromisoformat(iso_str.replace('Z', '+00:00'))\n",
        "                    # è½‰æˆ å°ç£æ™‚å€\n",
        "                    dt_tw = dt_utc.astimezone(TW_TZ)\n",
        "                    # ç§»é™¤æ™‚å€æ¨™ç±¤ (è®Šæˆ Naive) ä»¥ä¾¿å­˜å…¥ Excel\n",
        "                    return dt_tw.replace(tzinfo=None)\n",
        "                except: return None\n",
        "\n",
        "            # [ç­–ç•¥ A] JSON-LD (æœ€ç©©å®šçš„å¾Œé–€)\n",
        "            # Yahoo é€šå¸¸æœƒæŠŠè³‡æ–™è—åœ¨ <script type=\"application/ld+json\"> è£¡\n",
        "            scripts = soup.find_all('script', type='application/ld+json')\n",
        "            for script in scripts:\n",
        "                try:\n",
        "                    data = json.loads(script.string)\n",
        "                    # æœ‰æ™‚å€™æ˜¯ listï¼Œæœ‰æ™‚å€™æ˜¯ dict\n",
        "                    if isinstance(data, list):\n",
        "                        data = data[0]\n",
        "\n",
        "                    # æ‰¾ datePublished\n",
        "                    if 'datePublished' in data:\n",
        "                        dt = datetime.fromisoformat(data['datePublished'].replace('Z', '+00:00'))\n",
        "                        actual_date = dt.replace(tzinfo=None)\n",
        "                        break\n",
        "                except: continue\n",
        "\n",
        "            # [ç­–ç•¥ B] Meta Tag (å‚™ç”¨)\n",
        "            if not actual_date:\n",
        "                meta_time = soup.select_one(\"meta[property='article:published_time']\")\n",
        "                if meta_time and meta_time.get('content'):\n",
        "                    try:\n",
        "                        dt = datetime.fromisoformat(meta_time['content'].replace('Z', '+00:00'))\n",
        "                        actual_date = dt.replace(tzinfo=None)\n",
        "                    except: pass\n",
        "\n",
        "            # [ç­–ç•¥ C] UI é¡¯ç¤º (æœ€å¾Œæ‰‹æ®µ)\n",
        "            if not actual_date:\n",
        "                time_tag = soup.select_one(\".caas-attr-meta-time time\")\n",
        "                if time_tag and time_tag.has_attr('datetime'):\n",
        "                    try:\n",
        "                        dt = datetime.fromisoformat(time_tag['datetime'].replace('Z', '+00:00'))\n",
        "                        actual_date = dt.replace(tzinfo=None)\n",
        "                    except: pass\n",
        "\n",
        "        # --- å…¶ä»–ä¾†æºç¶­æŒåŸæ¨£ ---\n",
        "        elif source == \"è‡ªç”±æ™‚å ±\":\n",
        "            container = soup.select_one(\"div.text\")\n",
        "            if container:\n",
        "                for trash in container.select(\".promote, .author, .app_ad\"): trash.decompose()\n",
        "            time_tag = soup.select_one(\".content_header .time\")\n",
        "            if time_tag: actual_date = parse_relative_time(time_tag.text)\n",
        "\n",
        "        elif source == \"è¯åˆæ–°è\":\n",
        "            container = soup.select_one(\"section.article-content__editor\")\n",
        "            time_tag = soup.select_one(\"time.article-content__time\")\n",
        "            if time_tag: actual_date = parse_relative_time(time_tag.text)\n",
        "\n",
        "        elif source == \"ETtoday\":\n",
        "            container = soup.select_one(\"div.story\")\n",
        "            time_tag = soup.select_one(\"time[itemprop='datePublished']\")\n",
        "            if time_tag: actual_date = parse_relative_time(time_tag.text)\n",
        "\n",
        "        else:\n",
        "            container = soup.body\n",
        "\n",
        "        # --- 2. æå–æ–‡å­— ---\n",
        "        if container:\n",
        "            paragraphs = [p.text.strip() for p in container.find_all('p') if len(p.text) > 10]\n",
        "            content = \"\\n\".join(paragraphs)\n",
        "\n",
        "        if len(content) < 50:\n",
        "            paragraphs = [p.text.strip() for p in soup.find_all('p') if len(p.text) > 20]\n",
        "            content = \"\\n\".join(paragraphs)\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"   âš ï¸ è§£æå…§æ–‡å¤±æ•— ({url}): {e}\")\n",
        "        pass\n",
        "\n",
        "    return (content if len(content) > 50 else None), actual_date\n",
        "\n",
        "def is_date_valid(date_obj):\n",
        "    try:\n",
        "        cutoff_date = get_now_tw() - timedelta(days=DAYS_LIMIT)\n",
        "        # ç¢ºä¿å‚³å…¥çš„ date_obj ä¹Ÿæ˜¯ naive çš„ (é€™åœ¨ fetch_article_content æœƒè™•ç†)\n",
        "        if date_obj.tzinfo is not None:\n",
        "            date_obj = date_obj.replace(tzinfo=None)\n",
        "        return date_obj >= cutoff_date\n",
        "    except: return True\n",
        "\n"
      ],
      "metadata": {
        "id": "wtajLEBT1Xe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ã€€æ¨¡çµ„ C: é‡å°æ€§çˆ¬èŸ²è§£æå™¨ (Parsers)"
      ],
      "metadata": {
        "id": "bzFcVunVEWOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# å„å¤§æ–°èç¶²è§£æå™¨\n",
        "# ==========================================\n",
        "\n",
        "def crawl_ltn(keyword):\n",
        "    results = []\n",
        "    page = 1\n",
        "\n",
        "    while len(results) < LIMIT:\n",
        "        url = f\"https://search.ltn.com.tw/list?keyword={keyword}&page={page}\"\n",
        "        soup = fetch_page_robust(url, \"è‡ªç”±æ™‚å ±\")\n",
        "        if not soup: break\n",
        "\n",
        "        items = soup.select(\"ul.list > li\")\n",
        "        if not items: break\n",
        "\n",
        "        new_count = 0\n",
        "        for item in items:\n",
        "            if len(results) >= LIMIT: break\n",
        "            try:\n",
        "                time_tag = item.select_one(\"span.time\")\n",
        "                date_time = parse_relative_time(time_tag.text) if time_tag else datetime.now()\n",
        "\n",
        "                # å³æ™‚éæ¿¾ï¼šè‹¥æ—¥æœŸå¤ªèˆŠï¼Œè·³éä¸æ”¶éŒ„\n",
        "                if not is_date_valid(date_time): continue\n",
        "\n",
        "                link_tag = item.select_one(\"a.tit\")\n",
        "                if not link_tag: continue\n",
        "                title = link_tag.text.strip()\n",
        "                link = link_tag['href']\n",
        "\n",
        "                results.append({\"source\": \"è‡ªç”±æ™‚å ±\", \"title\": title, \"link\": link, \"date\": date_time, \"content\": title})\n",
        "                new_count += 1\n",
        "            except: continue\n",
        "\n",
        "        # å¦‚æœæ•´é éƒ½æ²’æ–°è³‡æ–™(ä¸”å·²æŠ“éè³‡æ–™)ï¼Œå¯èƒ½ä»£è¡¨å¾Œé¢éƒ½æ˜¯èˆŠèäº†\n",
        "        if new_count == 0 and len(results) > 0: break\n",
        "        if new_count == 0 and page > 3: break # å®‰å…¨æ©Ÿåˆ¶\n",
        "        page += 1\n",
        "\n",
        "    return results\n",
        "\n",
        "def crawl_udn(keyword):\n",
        "    results = []\n",
        "    page = 1\n",
        "\n",
        "    while len(results) < LIMIT:\n",
        "        url = f\"https://udn.com/search/word/2/{keyword}/{page}\"\n",
        "        soup = fetch_page_robust(url, \"è¯åˆæ–°è\")\n",
        "        if not soup: break\n",
        "\n",
        "        items = soup.select(\"div.story-list__text\")\n",
        "        if not items: break\n",
        "\n",
        "        new_count = 0\n",
        "        for item in items:\n",
        "            if len(results) >= LIMIT: break\n",
        "            try:\n",
        "                time_tag = item.select_one(\"time\")\n",
        "                date_time = parse_relative_time(time_tag.text) if time_tag else datetime.now()\n",
        "\n",
        "                # å³æ™‚éæ¿¾\n",
        "                if not is_date_valid(date_time): continue\n",
        "\n",
        "                h2_tag = item.select_one(\"h2 a\")\n",
        "                if not h2_tag: continue\n",
        "                title = h2_tag.text.strip()\n",
        "                link = h2_tag['href']\n",
        "                p_tag = item.select_one(\"p\")\n",
        "                content = p_tag.text.strip() if p_tag else title\n",
        "                results.append({\"source\": \"è¯åˆæ–°è\", \"title\": title, \"link\": link, \"date\": date_time, \"content\": content})\n",
        "                new_count += 1\n",
        "            except: continue\n",
        "\n",
        "        if new_count == 0 and len(results) > 0: break\n",
        "        if new_count == 0 and page > 3: break\n",
        "        page += 1\n",
        "\n",
        "    return results\n",
        "\n",
        "def crawl_ettoday(keyword):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ Google News RSS + site:ettoday.net æŒ‡ä»¤ã€‚\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    encoded_kw = urllib.parse.quote(keyword)\n",
        "\n",
        "    # åŠ å…¥ when:{DAYS_LIMIT}d è®“ Google å¹«å¿™ç¯©é¸æ™‚é–“\n",
        "    rss_url = f\"https://news.google.com/rss/search?q={encoded_kw}+site:ettoday.net+when:{DAYS_LIMIT}d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant\"\n",
        "\n",
        "    soup = fetch_page_robust(rss_url, \"ETtoday_RSS\")\n",
        "    if not soup: return []\n",
        "\n",
        "    items = soup.find_all(\"item\")\n",
        "\n",
        "    for item in items:\n",
        "        if len(results) >= LIMIT: break\n",
        "        try:\n",
        "            pub_date_str = item.pubDate.text.strip()\n",
        "            try:\n",
        "                # è§£æ RSS æ™‚é–“ (RFC 822 æ ¼å¼)\n",
        "                date_time = parsedate_to_datetime(pub_date_str)\n",
        "                # çµ±ä¸€è½‰ç‚ºç„¡æ™‚å€ (naive datetime) ä»¥ä¾¿æ¯”è¼ƒ\n",
        "                if date_time.tzinfo is not None:\n",
        "                    date_time = date_time.replace(tzinfo=None)\n",
        "            except:\n",
        "                date_time = datetime.now()\n",
        "\n",
        "            # å³æ™‚éæ¿¾èˆŠè\n",
        "            if not is_date_valid(date_time): continue\n",
        "\n",
        "            # ç§»é™¤æ¨™é¡Œå°¾ç¶´\n",
        "            title = item.title.text.strip().rsplit(\"-\", 1)[0].strip()\n",
        "            link = item.link.text.strip()\n",
        "\n",
        "            results.append({\n",
        "                \"source\": \"ETtoday\",\n",
        "                \"title\": title,\n",
        "                \"link\": link,\n",
        "                \"date\": date_time,\n",
        "                \"content\": title\n",
        "            })\n",
        "        except: continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def crawl_yahoo(keyword):\n",
        "    results = []\n",
        "    offset = 1\n",
        "\n",
        "    # Yahoo åŠ å…¥ sort=date åƒæ•¸ï¼Œè®“èˆŠèæ›´å®¹æ˜“è¢«éæ¿¾\n",
        "    base_url = \"https://tw.news.yahoo.com/search?p={}&fr2=p%3As%2Cv%3Ai%2Cm%3Asb-top&sort=date&b={}\"\n",
        "\n",
        "    while len(results) < LIMIT:\n",
        "        url = base_url.format(keyword, offset)\n",
        "        soup = fetch_page_robust(url, \"Yahoo\")\n",
        "        if not soup: break\n",
        "\n",
        "        main_area = soup.select_one(\"#Stream\") or soup.select_one(\"#main\") or soup\n",
        "        links_in_list = main_area.select(\"li a\")\n",
        "        if len(links_in_list) > 5:\n",
        "            all_links = links_in_list\n",
        "        else:\n",
        "            all_links = main_area.find_all(\"a\")\n",
        "\n",
        "        if not all_links: break\n",
        "\n",
        "        new_count = 0\n",
        "        seen_links = set()\n",
        "        title_blacklist = [\"ä¸‹è¼‰\", \"éš±ç§\", \"æ¢æ¬¾\", \"æ”¿ç­–\", \"å»£å‘Š\", \"æœå‹™\", \"ä¿¡ç®±\", \"è‚¡å¸‚\", \"æ°£è±¡\", \"é‹å‹•\", \"åäºº\",\n",
        "                           \"å½±éŸ³\", \"ç„¦é»\", \"é¦–é \", \"æœå°‹\", \"ç™»å…¥\", \"æŒ‡å—\", \"æ‡¶äººåŒ…\", \"Yahoo\", \"æ‹è³£\", \"è³¼ç‰©\"]\n",
        "\n",
        "        for link_tag in all_links:\n",
        "            if len(results) >= LIMIT: break\n",
        "            try:\n",
        "                title = link_tag.text.strip()\n",
        "                href = link_tag.get('href')\n",
        "\n",
        "                # --- V5 ç²¾æº–éæ¿¾ ---\n",
        "                if not href or not title: continue\n",
        "                if len(title) < 12: continue\n",
        "                if title.startswith(\"Yahoo\") and len(title) < 20: continue\n",
        "                if any(b_word in title for b_word in title_blacklist): continue\n",
        "                if \"/topic/\" in href or \"/tag/\" in href or \"/search\" in href: continue\n",
        "                if \".html\" not in href: continue\n",
        "                if href.startswith(\"/\"): href = \"https://tw.news.yahoo.com\" + href\n",
        "\n",
        "                # Yahoo åˆ—è¡¨é é€šå¸¸æ²’æ—¥æœŸï¼Œæˆ‘å€‘å‡è¨­å®ƒæ˜¯æ–°çš„ (å› ç‚ºæœ‰ sort=date)\n",
        "                # ä½†åœ¨å»é‡æª¢æŸ¥å¾Œå†åŠ å…¥\n",
        "                is_duplicate = False\n",
        "                for r in results:\n",
        "                    if r['link'] == href:\n",
        "                        is_duplicate = True\n",
        "                        break\n",
        "                if is_duplicate or href in seen_links: continue\n",
        "\n",
        "                seen_links.add(href)\n",
        "                results.append({\n",
        "                    \"source\": \"Yahooæ–°è\",\n",
        "                    \"title\": title,\n",
        "                    \"link\": href,\n",
        "                    \"date\": datetime.now(),\n",
        "                    \"content\": title\n",
        "                })\n",
        "                new_count += 1\n",
        "            except: continue\n",
        "\n",
        "        if new_count == 0: break\n",
        "        offset += 10\n",
        "        if offset > 50: break # å®‰å…¨æ©Ÿåˆ¶\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# ç¸½æ§ä¸­å¿ƒ\n",
        "# ==========================================\n",
        "\n",
        "def crawl_manager(nested_config):\n",
        "    all_data = []\n",
        "    print(f\"\\nğŸ•·ï¸ é–‹å§‹åŸ·è¡Œæ¨¹ç‹€çˆ¬å– (Limit={LIMIT}, Lookback={DAYS_LIMIT}å¤©)...\")\n",
        "\n",
        "    # å»ºç«‹åŸ·è¡Œç·’æ±  (Thread Pool)\n",
        "    # max_workers=4 å°æ‡‰ 4 å€‹ä¾†æº\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "\n",
        "        for topic, base_map in nested_config.items():\n",
        "            print(f\"\\nğŸ“‚ ä¸»é¡Œ: {topic}\")\n",
        "            for root_kw, search_terms in base_map.items():\n",
        "                print(f\"   ğŸŒ¿ æºé ­: {root_kw}\")\n",
        "\n",
        "                for search_kw in search_terms:\n",
        "                    prefix = \"      --\" if search_kw != root_kw else \"      ->\"\n",
        "                    print(f\"{prefix}æ­£åœ¨æœå°‹: {search_kw}\")\n",
        "\n",
        "                    # å®šç¾©è¦åŸ·è¡Œçš„ä»»å‹™æ¸…å–®\n",
        "                    # æ ¼å¼: executor.submit(å‡½å¼åç¨±, åƒæ•¸)\n",
        "                    future_to_source = {\n",
        "                        executor.submit(crawl_ltn, search_kw): \"LTN\",\n",
        "                        executor.submit(crawl_yahoo, search_kw): \"Yahoo\",\n",
        "                        executor.submit(crawl_udn, search_kw): \"UDN\",\n",
        "                        executor.submit(crawl_ettoday, search_kw): \"ETtoday\"\n",
        "                    }\n",
        "\n",
        "                    # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆä¸¦æ”¶é›†çµæœ\n",
        "                    for future in as_completed(future_to_source):\n",
        "                        source_name = future_to_source[future]\n",
        "                        try:\n",
        "                            data = future.result()\n",
        "\n",
        "                            # è£œä¸Šæ¨™ç±¤ (Tagging)\n",
        "                            for item in data:\n",
        "                                item['topic'] = topic\n",
        "                                item['root_keyword'] = root_kw\n",
        "                                item['keyword'] = search_kw\n",
        "\n",
        "                            all_data.extend(data)\n",
        "                        except Exception as e:\n",
        "                            print(f\"         âš ï¸ [{source_name}] ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "                    # ç¨å¾®ä¼‘æ¯ï¼Œé¿å…ç¬é–“è«‹æ±‚éå¤šå°è‡´ Colab ç¶²è·¯å¡è»Š\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "    print(f\"\\nâœ… å…¨æ•¸çˆ¬å–å®Œæˆï¼Œå…±æ”¶é›† {len(all_data)} å‰‡è³‡æ–™ã€‚\")\n",
        "    return pd.DataFrame(all_data)"
      ],
      "metadata": {
        "id": "WINScSBeEYx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡çµ„ D: è³‡æ–™æ¸…æ´—èˆ‡ AI æ‰¹é‡è™•ç† (Clean & AI Batch)"
      ],
      "metadata": {
        "id": "Xey2E3KuEjaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "import difflib\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "TW_TZ = timezone(timedelta(hours=8))\n",
        "\n",
        "def get_now_tw():\n",
        "    \"\"\"å–å¾—å°ç£ç•¶ä¸‹æ™‚é–“ (Naive)\"\"\"\n",
        "    return datetime.now(TW_TZ).replace(tzinfo=None)\n",
        "\n",
        "def clean_and_filter_data(df, lookback_days):\n",
        "    \"\"\"\n",
        "    1. æ™‚é–“ç¯©é¸\n",
        "    2. ç¶²å€å®Œå…¨å»é‡\n",
        "    3. æ¨™é¡Œæ¨¡ç³Šå»é‡ (Fuzzy Deduplication)\n",
        "    \"\"\"\n",
        "    if df.empty: return df\n",
        "\n",
        "    # 1. æ™‚é–“ç¯©é¸\n",
        "    cutoff_date = get_now_tw() - timedelta(days=lookback_days)\n",
        "    df = df[df['date'] >= cutoff_date]\n",
        "\n",
        "    # 2. ç¶²å€å®Œå…¨å»é‡\n",
        "    df = df.drop_duplicates(subset=['link'])\n",
        "\n",
        "    # 3. æ¨™é¡Œæ¨¡ç³Šå»é‡æ¼”ç®—æ³•\n",
        "    # ç›®çš„ï¼šéæ¿¾æ‰ã€Œæ›æ¹¯ä¸æ›è—¥ã€çš„é«˜åº¦ç›¸ä¼¼æ¨™é¡Œ\n",
        "    print(f\"ğŸ§¹ é–‹å§‹æ¸…æ´—... (åŸå§‹ç­†æ•¸: {len(df)})\")\n",
        "\n",
        "    # å…ˆå°‡æ¨™é¡Œè½‰ç‚ºåˆ—è¡¨\n",
        "    titles = df['title'].tolist()\n",
        "    keep_indices = []\n",
        "    seen_titles = []\n",
        "\n",
        "    for i, title in enumerate(titles):\n",
        "        is_duplicate = False\n",
        "        # èˆ‡å·²ç¶“ä¿ç•™çš„æ¨™é¡Œé€²è¡Œæ¯”å°\n",
        "        for seen in seen_titles:\n",
        "            # è¨ˆç®—ç›¸ä¼¼åº¦ (0.0 ~ 1.0)\n",
        "            similarity = difflib.SequenceMatcher(None, title, seen).ratio()\n",
        "\n",
        "            # é–€æª»è¨­ç‚º 0.7 (70% åƒå°±ç•¶ä½œé‡è¤‡)\n",
        "            if similarity > 0.7:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            keep_indices.append(df.index[i])\n",
        "            seen_titles.append(title)\n",
        "\n",
        "    # åªä¿ç•™æ²’æœ‰é‡è¤‡çš„åˆ—\n",
        "    df_clean = df.loc[keep_indices].reset_index(drop=True)\n",
        "\n",
        "    removed_count = len(df) - len(df_clean)\n",
        "    print(f\"ğŸ§¹ æ¸…æ´—å®Œæˆï¼šç§»é™¤äº† {removed_count} å‰‡é«˜åº¦ç›¸ä¼¼æˆ–èˆŠçš„æ–°èï¼Œå‰©é¤˜ {len(df_clean)} å‰‡ã€‚\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def process_yahoo_row(row_data):\n",
        "    idx, original_content, original_date, url, source = row_data\n",
        "\n",
        "    # é Yahoo ç›´æ¥å›å‚³åŸå€¼ (ç§’é)\n",
        "    if source != \"Yahooæ–°è\":\n",
        "        return idx, original_content, original_date, False\n",
        "\n",
        "    try:\n",
        "        # å‘¼å« Module B çš„æ–°å‡½å¼\n",
        "        full_text, actual_date = fetch_article_content(url, source)\n",
        "    except NameError:\n",
        "        full_text, actual_date = None, None\n",
        "\n",
        "    final_content = full_text if full_text else original_content\n",
        "    final_date = actual_date if actual_date else original_date\n",
        "\n",
        "    # æª¢æŸ¥æ˜¯å¦æœ‰æ ¡æ­£ (èª¤å·®è¶…é 1 å¤©)\n",
        "    is_corrected = False\n",
        "    if actual_date and abs((actual_date - original_date).days) > 0:\n",
        "        is_corrected = True\n",
        "\n",
        "    return idx, final_content, final_date, is_corrected\n",
        "\n",
        "def enrich_data_with_full_text(df):\n",
        "    \"\"\"\n",
        "    [å¹³è¡ŒåŠ é€Ÿç‰ˆ] å…§æ–‡è£œå®Œ\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ“– å•Ÿå‹•å…§æ–‡è£œå®Œ (åƒ…é‡å° Yahoo æ–°èæ›´æ–°)...\")\n",
        "\n",
        "    results_map = {}\n",
        "    date_corrected_count = 0\n",
        "\n",
        "    # æº–å‚™ä»»å‹™æ•¸æ“š\n",
        "    tasks_data = []\n",
        "    for i, row in df.iterrows():\n",
        "        tasks_data.append((i, row['content'], row['date'], row['link'], row['source']))\n",
        "\n",
        "    total = len(tasks_data)\n",
        "    completed = 0\n",
        "\n",
        "    # é–‹å•Ÿ 5 å€‹åŸ·è¡Œç·’\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = [executor.submit(process_yahoo_row, task) for task in tasks_data]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                idx, content, date, corrected = future.result()\n",
        "                results_map[idx] = (content, date)\n",
        "                if corrected: date_corrected_count += 1\n",
        "            except Exception: pass\n",
        "\n",
        "            completed += 1\n",
        "            if completed % 50 == 0:\n",
        "                print(f\"   ...é€²åº¦: {completed}/{total}\")\n",
        "\n",
        "    # å›å¡« DataFrame\n",
        "    new_contents = []\n",
        "    new_dates = []\n",
        "    for i in df.index:\n",
        "        if i in results_map:\n",
        "            new_contents.append(results_map[i][0])\n",
        "            new_dates.append(results_map[i][1])\n",
        "        else:\n",
        "            new_contents.append(df.loc[i, 'content'])\n",
        "            new_dates.append(df.loc[i, 'date'])\n",
        "\n",
        "    df['content'] = new_contents\n",
        "    df['date'] = new_dates\n",
        "\n",
        "    print(f\"âœ… è™•ç†å®Œç•¢ã€‚\")\n",
        "    print(f\"ğŸ•’ æ™‚é–“æ ¡æ­£ï¼šä¿®æ­£äº† {date_corrected_count} å‰‡ Yahoo èˆŠèã€‚\\n\")\n",
        "    return df\n",
        "\n",
        "def get_ai_response_batch_robust(news_chunk, max_retries=3):\n",
        "    \"\"\"\n",
        "    å°‡ã€Œä¸»é¡Œ (Topic)ã€ä»¥åŠã€Œé—œéµå­—ã€ç´å…¥åˆ¤æ–·æ¨™æº–ï¼Œ\n",
        "    å¼·èª¿ã€Œæ¨™é¡Œã€èˆ‡ã€Œé—œéµäº‹å¯¦æå–ã€ï¼Œé¿å… AI å¯«å‡ºç©ºæ³›æ‘˜è¦ã€‚\n",
        "    \"\"\"\n",
        "    if not news_chunk: return []\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. è³‡æ–™å‰è™•ç† (Data Preparation)\n",
        "    # ==========================================\n",
        "\n",
        "    data_entries = []\n",
        "    for item in news_chunk:\n",
        "        topic = item.get('topic', 'æœªæŒ‡å®š')\n",
        "        root = item.get('root_keyword', item.get('keyword', 'æœªæŒ‡å®š'))\n",
        "\n",
        "        # é è¦½æ–‡å­—æ¸…ç† (é¿å…æ›è¡Œç¬¦è™Ÿç ´å£ Prompt çµæ§‹)\n",
        "        content_preview = str(item.get('content', ''))[:400].replace('\\n', ' ')\n",
        "\n",
        "        entry = (\n",
        "            f\"ID: {item['id']}\\n\"\n",
        "            f\"[é ˜åŸŸ: {topic}] [é—œéµå­—: {root}]\\n\"\n",
        "            f\"æ¨™é¡Œ: {item['title']}\\n\"\n",
        "            f\"é è¦½: {content_preview}\\n\"\n",
        "            f\"{'-' * 20}\"\n",
        "        )\n",
        "        data_entries.append(entry)\n",
        "\n",
        "    # å°‡æ‰€æœ‰æ–°èçµ„åˆæˆä¸€å€‹å¤§å­—ä¸²\n",
        "    input_data_text = \"\\n\".join(data_entries)\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. æç¤ºè©å»ºæ§‹ (Prompt Engineering)\n",
        "    # ==========================================\n",
        "    prompt_text = f\"\"\"\n",
        "    ä½ æ˜¯ä¸€åæ¥µåº¦åš´è¬¹ä¸”èƒ½å¤ å®Œæ•´è®€æ‡‚æ–°èçš„ã€Œæ–°èå…§å®¹åˆ†æå°ˆå®¶ã€èˆ‡ã€Œç”¢æ¥­æƒ…å ±åˆ†æå¸«ã€ã€‚\n",
        "    ä½ çš„ä»»å‹™æ˜¯åœ¨è³‡è¨Šæœ‰é™çš„æƒ…æ³ä¸‹ï¼Œä¸­ç«‹ä¸”å®¢è§€çš„ç²¾æº–åˆ¤æ–·æ–°èåƒ¹å€¼ï¼Œå¾é›œäº‚çš„è¨Šæ¯ä¸­ç¯©é¸å‡ºå…·å‚™ã€Œå•†æ¥­æ±ºç­–åƒ¹å€¼ã€çš„æƒ…å ±ã€‚\n",
        "    è«‹åˆ†ææ–°èçš„èªå¢ƒæ˜¯å¦ç¬¦åˆä¸»é¡ŒåŠé—œéµå­—ï¼Œä¸¦åšå‡ºæ‘˜è¦èˆ‡æƒ…æ„Ÿåˆ¤æ–·ï¼Œåšå¾—å¥½æœƒåŠ å¹´çµ‚çé‡‘ã€‚\n",
        "\n",
        "    ã€ä»»å‹™ä¸€ï¼šé‚è¼¯èˆ‡èªå¢ƒå¯©æŸ¥ (Pass/Fail)ã€‘\n",
        "    è«‹å°ç…§ã€Œä¸»é¡Œ(Topic)ã€èˆ‡ã€Œé—œéµå­—(RootKW)ã€ï¼Œé€²è¡Œä»¥ä¸‹é‚è¼¯æª¢é©—ã€‚\n",
        "    **åˆ¤æ–·æ–°èåœ¨å¸¸è­˜ä¸Šæ˜¯å¦ã€ŒåŒæ™‚ã€ç¬¦åˆã€Œä¸»é¡Œ(Topic)ã€èˆ‡ã€Œé—œéµå­—(RootKW)ã€çš„ç¯„ç–‡ã€‚**\n",
        "      - âœ… é€šé: ä¸»é¡Œã€Œé¤é£²ã€ã€é—œéµå­—ã€Œæ˜Ÿå·´å…‹ã€ï¼Œæ–°èè¬›è¿°æ˜Ÿå·´å…‹æ¨å‡ºæ–°å“ã€å±•åº—ç­–ç•¥ã€è²¡å ±ã€‚\n",
        "      - âœ… é€šé: ä¸»é¡Œã€ŒåŠå°é«”ç”¢æ¥­ã€ã€é—œéµå­—ã€Œå°ç©é›»ã€ï¼Œæ–°èè¬›è¿°å°ç©é›»åœ¨æ—¥æœ¬æ“´å» ã€å¤§é‡æ‹›å‹Ÿå·¥ç¨‹å¸«ã€‚\n",
        "      - âœ… é€šé: ä¸»é¡Œã€ŒAIæ‡‰ç”¨ã€ã€é—œéµå­—ã€ŒGOOGLEã€ï¼Œæ–°èè¬›è¿°GEMINI 3ç™¼å¸ƒã€NANO BANANAæ”¹è®Šå‰µä½œå·¥å…·ã€‚\n",
        "      - âœ… é€šé: ä¸»é¡Œã€Œåœ°æ–¹å‰µç”Ÿã€ã€é—œéµå­—ã€Œå°å—ã€ï¼Œæ–°èè¬›è¿°å°å—åœ¨åœ°è—è¡“å®¶åˆ©ç”¨ç«¹å­è£½ä½œå®¶é„‰å…¥å£å¤§å‹è£ç½®è—è¡“ã€‚\n",
        "      - âœ… é€šé: ä¸»é¡Œã€Œè©é¨™çŠ¯ç½ªã€ã€é—œéµå­—ã€ŒåŠ å¯†è²¨å¹£ã€ï¼Œæ–°èè¬›è¿°72äººé‡å‡å¹£å•†è¢«å‰2å±¤çš®æå¤±è¿‘7000è¬ è©åœ˜é¦–è…¦åˆ¥å¢…è¢«æ‰£èµ·è¨´æ±‚åˆ‘20å¹´ã€‚\n",
        "\n",
        "    **è‹¥ç¬¦åˆä»»ä¸€å‰”é™¤è¦å‰‡ï¼Œè«‹æ¨™è¨˜ `is_relevant: false`ï¼š**\n",
        "    1. âŒ **ä¸»è§’åˆ¤å®š (Role Check)**ï¼š\n",
        "      - é—œéµå­—åƒ…æ˜¯ã€Œæ¡ˆç™¼åœ°é»ã€æˆ–ã€ŒèƒŒæ™¯é“å…·ã€ï¼Œä¸¦éä¸»è¦è§’è‰²ã€‚\n",
        "        (ä¾‹ï¼šä¸»é¡Œã€Œé¤é£²ã€/é—œéµå­—ã€Œæ˜Ÿå·´å…‹ã€-> æ–°èã€Œè»Šæ‰‹åœ¨æ˜Ÿå·´å…‹é¢äº¤è¢«æ•ã€ -> å‰”é™¤)\n",
        "      - é—œéµå­—åƒ…æ˜¯ã€Œæ¯”å–»ã€æˆ–ã€Œå‹è™Ÿã€ã€ã€Œå¦ä¸€å€‹åè©çš„éƒ¨åˆ†ç‰‡æ®µã€ã€‚\n",
        "        (ä¾‹ï¼šä¸»é¡Œã€ŒAIã€ -> æ–°èã€ŒiPhone Air è¼•è–„ä¸Šå¸‚ã€ -> å‰”é™¤)\n",
        "    2. âŒ **é ˜åŸŸéŒ¯ç½® (Domain Mismatch)**ï¼š\n",
        "      - ä¸»é¡Œæ˜¯ã€Œæ¯æ—¥å¤©æ°£ã€ï¼Œé—œéµå­—æ˜¯ã€Œåœ¨åœ°æ°£è±¡ã€ï¼Œä½†æ–°èå…§å®¹æ˜¯é‡è¦æ´»å‹•éœ€è¦åšå¤©æ°£è©•ä¼°ã€‚\n",
        "        (ä¾‹ï¼šéŸ“åœ‹å³å°‡ç™¼å°„è‡ªè£½ç«ç®­ï¼Œæ–°èå…§å®¹è¬›è¿°ç«ç®­ç™¼å°„çš„æ°£è±¡æ¢ä»¶è©•ä¼°ã€‚)\n",
        "      - ä¸»é¡Œæ˜¯ã€Œç”¢æ¥­/ç§‘æŠ€ã€ï¼Œä½†æ–°èå…§å®¹å…¶å¯¦æ˜¯ã€Œæ”¿æ²»å£æ°´ã€æˆ–ã€Œç¤¾æœƒåˆ‘æ¡ˆã€ã€‚\n",
        "        (ä¾‹ï¼šæ–°èä¸»è»¸æ˜¯å€™é¸äººé€ å‹¢å–Šå£è™Ÿèªªè¦ç™¼å±•AIï¼Œæˆ–æ”¿æ²»äººç‰©äº’ç›¸æ”»æ“Šå­¸æ­·é€ å‡ï¼Œè€Œéæ¢è¨ç”¢æ¥­æœ¬èº«ã€‚)\n",
        "      - ä¸»é¡Œæ˜¯ã€Œé‡‘è/åŠ å¯†è²¨å¹£ã€ï¼Œé—œéµå­—ç‚ºã€Œæ¯”ç‰¹å¹£ã€ï¼Œä½†æ–°èå…§å®¹å…¶å¯¦æ˜¯ã€Œè©é¨™æ‰‹æ³•ã€æˆ–ã€Œç¤¾æœƒæ¡ˆä»¶ã€ã€‚\n",
        "        (ä¾‹ï¼šæ–°èä¸»è»¸æ˜¯è­¦æ–¹ç ´ç²æ´—éŒ¢é›†åœ˜ï¼ŒåŠ å¯†è²¨å¹£åƒ…æ˜¯ä½œç‚ºè´“æ¬¾çš„ç§»è½‰å·¥å…·ï¼Œä¸¦éæ¢è¨å¹£åœˆç”Ÿæ…‹æˆ–æŠ€è¡“ã€‚)\n",
        "    3. âŒ **åƒ¹å€¼ä½è½ (Low Quality)**ï¼š\n",
        "      - ä¸»é¡Œæ˜¯ã€ŒéŠæˆ²/é›»ç«¶ã€ï¼Œé—œéµå­—æ˜¯ã€ŒMORPGã€ä½†æ–°èå…§å®¹å…¶å¯¦æ˜¯ã€Œå–®ç´”æ”»ç•¥ã€ã€ã€Œç›´æ’­ä¸»å…«å¦ã€æˆ–ã€Œå¤–æ›æ•™å­¸ã€ã€‚\n",
        "        (ä¾‹ï¼šæ–°èæ•™ç©å®¶å¦‚ä½•é€šé—œæŸå€‹å‰¯æœ¬ï¼Œæˆ–æ˜¯æŸå¯¦æ³ä¸»çš„ç§äººæ„Ÿæƒ…ç³¾ç´›ã€‚)\n",
        "      - ä¸»é¡Œæ˜¯ã€Œè‚¡å¸‚/å€‹è‚¡ã€ï¼Œä½†æ–°èå…§å®¹å…¶å¯¦æ˜¯ã€Œå¤§ç›¤æµæ°´å¸³ã€ã€‚\n",
        "        (ä¾‹ï¼šæ–°èåƒ…åˆ—å‡ºç•¶æ—¥å°è‚¡æ¼²è·Œå¹…æ’è¡Œæ¦œï¼Œå°ç©é›»åƒ…åœ¨åˆ—è¡¨ä¸­è¢«é †å¸¶æåŠæ”¶ç›¤åƒ¹ï¼Œç„¡å…·é«”ç‡Ÿé‹åˆ†æã€‚)\n",
        "      - ä¸»é¡Œæ˜¯ã€Œå“ç‰Œå‹•æ…‹ã€ï¼Œä½†æ–°èä¸»è»¸æ˜¯ã€Œé¡§å®¢/ç¶²ç´…åœ¨åº—å…§çš„è„«åºè¡Œç‚ºã€ã€ã€Œå¥§å®¢ç³¾ç´›ã€ã€ã€Œç§äººæ©æ€¨ã€æˆ–ã€Œå¦¨ç¤™é¢¨åŒ–ã€ã€‚\n",
        "        (ä¾‹ï¼šç¶²ç´…åœ¨è³£å ´è£¸éœ²ã€é¡§å®¢åœ¨è¶…å•†æ‰“æ¶ã€åœè»Šå ´åµæ¶ã€‚)\n",
        "      - **åˆ¤æ–·ä¾æ“š**ï¼šé™¤éè©²äº‹ä»¶å¼•ç™¼äº†ã€Œé‡å¤§ç‡Ÿé‹æ”¿ç­–æ”¹è®Šã€æˆ–ã€Œè‚¡åƒ¹æ³¢å‹•ã€ï¼Œå¦å‰‡è¦–ç‚ºå–®ç´”ç¤¾æœƒæ¡ˆä»¶ã€‚\n",
        "    4. âŒ **ç„¡æ•ˆè³‡è¨Š (Invalid)**ï¼š\n",
        "      - é è¦½å…§å®¹é¡¯ç¤ºç‚ºå»£å‘Šã€ç™»å…¥é é¢ã€èª²ç¨‹æ¨éŠ·ã€æ¦œå–®æŸ¥è©¢é€£çµã€‚\n",
        "\n",
        "    ã€ä»»å‹™äºŒï¼šæ‘˜è¦ (Summary)ã€‘\n",
        "    **50-80å­—ç¹é«”ä¸­æ–‡æ‘˜è¦ï¼Œç”±æ–¼æ²’æœ‰å®Œæ•´å…§æ–‡ï¼Œè«‹åŸ·è¡Œã€Œé è¦½é©…å‹•ã€çš„æ‘˜è¦ç­–ç•¥ï¼š**\n",
        "    1. **é è¦½æ˜¯å¤§åŸå‰‡**ï¼šè«‹ä»¥é è¦½æ–‡å­—ç‚ºä¸»é€²è¡Œæ’°å¯«æ‘˜è¦ï¼Œä½†è‹¥é è¦½æ–‡å­—è¢«æˆªæ–·æˆ–èªæ„ä¸æ¸…ï¼Œè«‹ä»¥ã€Œæ¨™é¡Œã€ä½œç‚ºè¼”åŠ©ã€‚\n",
        "    2. **å»å½¢å®¹è©åŒ–**ï¼šåˆªé™¤ã€Œå·¨å¤§çš„ã€ã€ã€Œä»¤äººéœ‡é©šçš„ã€ç­‰æƒ…ç·’å­—çœ¼ï¼Œåªä¿ç•™ã€Œäººã€äº‹ã€æ™‚ã€åœ°ã€ç‰©ã€æ•¸æ“šã€ã€‚\n",
        "    3. **ç²¾æº–èªªå®Œä½†è£œå……æ‰€éœ€ç´°ç¯€**ï¼šå…ˆç›´æ¥é™³è¿°çµæœï¼Œå†è£œå……ç´°ç¯€ã€åŸå› ã€éç¨‹ã€‚(ä¾‹ï¼šã€Œå°ç©é›»ç†Šæœ¬å» å®£ä½ˆå¹´åº•é‡ç”¢ï¼Œæœˆç”¢èƒ½é”5è¬ç‰‡ã€‚ã€)\n",
        "\n",
        "    ã€ä»»å‹™ä¸‰ï¼šæƒ…æ„Ÿåˆ¤æ–· (Sentiment)ã€‘\n",
        "    åˆ¤æ–·æ–°èå°è©²ã€Œé—œéµå­—ã€æ˜¯æ­£é¢(åˆ©å¤š/æˆé•·)ã€è² é¢(åˆ©ç©º/è¡°é€€/çˆ­è­°)é‚„æ˜¯ä¸­ç«‹(å®¢è§€å ±å°)ã€‚\n",
        "\n",
        "    ã€è¼¸å…¥è³‡æ–™ã€‘\n",
        "    {input_data_text}\n",
        "\n",
        "    è«‹å›å‚³ JSON List (åˆ‡å‹¿åŒ…å« Markdown æ¨™è¨˜):\n",
        "    [{{\n",
        "        \"id\": 0,\n",
        "        \"is_relevant\": True/False,\n",
        "        \"reason\": \"é€™å‰‡æ–°èæ”¶å…¥èˆ‡å¦ã€æ˜¯å¦ç¬¦åˆæ­¤æ™‚è¨­å®šçš„ã€Œä¸»é¡Œ(Topic)ã€èˆ‡ã€Œé—œéµå­—(RootKW)ã€ï¼Œä¸¦ç°¡è¿°ç†ç”±(ç„¡è«– True/False çš†é ˆå¡«å¯«æ­¤æ¬„)\",\n",
        "        \"summary\": \"æ‘˜è¦å…§å®¹(åŒ…å«ï¼šäººã€äº‹ã€æ™‚ã€åœ°ã€ç‰©ã€æ•¸æ“šï¼Œä¸¦é©ç•¶è£œå……ç´°ç¯€)...\",\n",
        "        \"sentiment\": \"ä¸­ç«‹/æ­£é¢/è² é¢\"\n",
        "    }}]\n",
        "    \"\"\"\n",
        "    # ==========================================\n",
        "    # 3. å‘¼å« API (Execution)\n",
        "    # ==========================================\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.1, # é™ä½éš¨æ©Ÿæ€§ï¼Œè®“åˆ¤æ–·æ›´åš´è¬¹\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt_text, generation_config=generation_config)\n",
        "            response_text = response.text.strip()\n",
        "            if response_text.startswith(\"```\"):\n",
        "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "            return json.loads(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            wait_time = (attempt + 1) * 2\n",
        "            print(f\"      âš ï¸ AI Batch å¤±æ•— ({e})ï¼Œé‡è©¦ä¸­ ({wait_time}s)...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    print(f\"      âŒ AI Batch å¾¹åº•å¤±æ•—ã€‚\")\n",
        "    return []\n",
        "\n",
        "def run_ai_pipeline(df):\n",
        "    if df.empty: return df\n",
        "    if not ENABLE_AI_SUMMARY: return df\n",
        "\n",
        "    # ä»¥é€£çµè®€å–å®Œæ•´æ–°èï¼Œç¢ºèªæ™‚é–“\n",
        "    df = enrich_data_with_full_text(df)\n",
        "\n",
        "    try: lookback = LOOKBACK_DAYS\n",
        "    except: lookback = 2\n",
        "\n",
        "    now_tw = datetime.now(TW_TZ).replace(tzinfo=None)\n",
        "    cutoff_date = now_tw - timedelta(days=lookback)\n",
        "\n",
        "    count_before = len(df)\n",
        "    df = df[df['date'] >= cutoff_date].reset_index(drop=True)\n",
        "    dropped = count_before - len(df)\n",
        "\n",
        "    if dropped > 0:\n",
        "        print(f\"ğŸ§¹ äºŒæ¬¡æ¸…æ´—ï¼šç§»é™¤äº† {dropped} å‰‡æ ¡æ­£å¾Œç™¼ç¾éæœŸçš„æ–°èã€‚\")\n",
        "\n",
        "    if df.empty: return df\n",
        "\n",
        "    print(\"ğŸ¤– é–‹å§‹ AI æ‰¹é‡æ‘˜è¦èˆ‡å¯©æŸ¥...\")\n",
        "\n",
        "    df['temp_id'] = range(len(df))\n",
        "    df['ai_summary'] = \"\"\n",
        "    df['sentiment'] = \"\"\n",
        "    df['is_relevant'] = True\n",
        "    df['relevance_reason'] = \"\"\n",
        "\n",
        "    records = df.to_dict('records')\n",
        "\n",
        "    for i in range(0, len(records), BATCH_SIZE):\n",
        "        chunk = records[i:i+BATCH_SIZE]\n",
        "        for item in chunk: item['id'] = item['temp_id']\n",
        "\n",
        "        print(f\"   Processing batch {i//BATCH_SIZE + 1} / {(len(records)-1)//BATCH_SIZE + 1} ...\")\n",
        "\n",
        "        ai_results = get_ai_response_batch_robust(chunk)\n",
        "\n",
        "        for res in ai_results:\n",
        "            idx = res.get('id')\n",
        "            if idx is not None:\n",
        "                mask = df['temp_id'] == idx\n",
        "                df.loc[mask, 'ai_summary'] = res.get('summary', 'æ‘˜è¦å¤±æ•—')\n",
        "                df.loc[mask, 'sentiment'] = res.get('sentiment', 'ä¸­ç«‹')\n",
        "\n",
        "                is_rel = res.get('is_relevant')\n",
        "                if isinstance(is_rel, str): is_rel = is_rel.lower() == 'true'\n",
        "\n",
        "                df.loc[mask, 'is_relevant'] = is_rel if is_rel is not None else True\n",
        "                df.loc[mask, 'relevance_reason'] = res.get('reason', '')\n",
        "\n",
        "        time.sleep(4)\n",
        "\n",
        "    return df.drop(columns=['temp_id'])"
      ],
      "metadata": {
        "id": "59z9bEL2Eo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡çµ„ E: å ±å‘Šç”Ÿæˆ (Output)"
      ],
      "metadata": {
        "id": "92AfOrPyE1QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment, Font, PatternFill, Border, Side\n",
        "from openpyxl.utils import get_column_letter\n",
        "\n",
        "def get_taiwan_time():\n",
        "    tz_tw = timezone(timedelta(hours=8))\n",
        "    return datetime.now(tz_tw)\n",
        "\n",
        "def export_report_optimized(df, folder_path):\n",
        "    \"\"\"\n",
        "    [ç²¾æº–æ’ç‰ˆç‰ˆ] è¼¸å‡º Excel\n",
        "    ç‰¹è‰²ï¼šé‡å°é•·æ–‡å­—æ¬„ä½ (æ¨™é¡Œ/æ‘˜è¦/è©•èª) è¨­å®šå›ºå®šå¯¬åº¦ + è‡ªå‹•æ›è¡Œ\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"ç„¡è³‡æ–™å¯è¼¸å‡ºã€‚\")\n",
        "        return\n",
        "\n",
        "    # --- 1. æ¬„ä½ç¯©é¸èˆ‡æ›´å ---\n",
        "    columns_map = {\n",
        "        'topic': 'ç›£æ¸¬ä¸»é¡Œ',\n",
        "        'root_keyword': 'é—œéµå­—',\n",
        "        'date': 'ç™¼å¸ƒæ™‚é–“',\n",
        "        'source': 'ä¾†æº',\n",
        "        'title': 'æ–°èæ¨™é¡Œ',\n",
        "        'ai_summary': 'AI é‡é»æ‘˜è¦',\n",
        "        'sentiment': 'æƒ…ç·’',\n",
        "        'relevance_reason': 'AI è©•èª',\n",
        "        'link': 'é€£çµ'\n",
        "    }\n",
        "\n",
        "    available_cols = [c for c in columns_map.keys() if c in df.columns]\n",
        "    df_export = df[available_cols].copy()\n",
        "    df_export.rename(columns=columns_map, inplace=True)\n",
        "\n",
        "    # æ™‚é–“æ ¼å¼å„ªåŒ–\n",
        "    if 'ç™¼å¸ƒæ™‚é–“' in df_export.columns:\n",
        "        df_export['ç™¼å¸ƒæ™‚é–“'] = pd.to_datetime(df_export['ç™¼å¸ƒæ™‚é–“']).dt.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "    # --- 2. å­˜æª” ---\n",
        "    now_tw = get_taiwan_time()\n",
        "    file_name = f\"News_Report_{now_tw.strftime('%Y%m%d_%H%M')}_.xlsx\"\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
        "        df_export.to_excel(writer, sheet_name=\"ç¸½è¦½\", index=False)\n",
        "        if 'ç›£æ¸¬ä¸»é¡Œ' in df_export.columns:\n",
        "            topics = df_export['ç›£æ¸¬ä¸»é¡Œ'].unique()\n",
        "            for t in topics:\n",
        "                safe_name = str(t)[:30].replace(\"/\", \"_\")\n",
        "                sub_df = df_export[df_export['ç›£æ¸¬ä¸»é¡Œ'] == t]\n",
        "                sub_df.to_excel(writer, sheet_name=safe_name, index=False)\n",
        "\n",
        "    # --- 3. è¦–è¦ºç¾åŒ– (Style Formatting) ---\n",
        "    wb = load_workbook(file_path)\n",
        "\n",
        "    # é€šç”¨æ¨£å¼å®šç¾©\n",
        "    header_font = Font(bold=True, color=\"FFFFFF\")\n",
        "    header_fill = PatternFill(start_color=\"4F81BD\", end_color=\"4F81BD\", fill_type=\"solid\")\n",
        "    border_style = Side(style='thin')\n",
        "    border = Border(left=border_style, right=border_style, top=border_style, bottom=border_style)\n",
        "\n",
        "    # [é—œéµè¨­å®š] æ¯ä¸€æ¬„çš„å¯¬åº¦èˆ‡æ›è¡Œè¦å‰‡\n",
        "    # Key æ˜¯ Excel çš„æ¬„ä½ä»£è™Ÿ (A, B, C...)\n",
        "    column_settings = {\n",
        "        'A': {'width': 12, 'wrap': False}, # ç›£æ¸¬ä¸»é¡Œ\n",
        "        'B': {'width': 12, 'wrap': False}, # é—œéµå­—\n",
        "        'C': {'width': 18, 'wrap': False}, # ç™¼å¸ƒæ™‚é–“\n",
        "        'D': {'width': 10, 'wrap': False}, # ä¾†æº\n",
        "        'E': {'width': 40, 'wrap': True},  # [æ¨™é¡Œ] å¯¬åº¦å›ºå®š 40 + è‡ªå‹•æ›è¡Œ\n",
        "        'F': {'width': 40, 'wrap': True},  # [æ‘˜è¦] å¯¬åº¦å›ºå®š 40 + è‡ªå‹•æ›è¡Œ\n",
        "        'G': {'width': 8,  'wrap': False}, # æƒ…ç·’\n",
        "        'H': {'width': 40, 'wrap': True},  # [è©•èª] å¯¬åº¦å›ºå®š 40 + è‡ªå‹•æ›è¡Œ\n",
        "        'I': {'width': 15, 'wrap': False}  # é€£çµ (é€šå¸¸ä¸æ›è¡Œï¼Œä¿æŒå–®è¡Œé»æ“Š)\n",
        "    }\n",
        "\n",
        "    for sheet in wb.worksheets:\n",
        "        # å¥—ç”¨æ¬„å¯¬\n",
        "        for col_letter, settings in column_settings.items():\n",
        "            sheet.column_dimensions[col_letter].width = settings['width']\n",
        "\n",
        "        # å¥—ç”¨å„²å­˜æ ¼æ¨£å¼\n",
        "        for row in sheet.iter_rows():\n",
        "            for cell in row:\n",
        "                cell.border = border\n",
        "                col_letter = get_column_letter(cell.column)\n",
        "\n",
        "                # å–å¾—è©²æ¬„ä½çš„è¨­å®š (è‹¥ç„¡è¨­å®šå‰‡é è¨­ä¸æ›è¡Œ)\n",
        "                col_config = column_settings.get(col_letter, {'wrap': False})\n",
        "\n",
        "                # æ¨™é¡Œåˆ—\n",
        "                if cell.row == 1:\n",
        "                    cell.font = header_font\n",
        "                    cell.fill = header_fill\n",
        "                    cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "                # å…§å®¹åˆ—\n",
        "                else:\n",
        "                    # [æ ¸å¿ƒ] è¨­å®šè‡ªå‹•æ›è¡Œ (Wrap Text)\n",
        "                    cell.alignment = Alignment(\n",
        "                        vertical='top',\n",
        "                        wrap_text=col_config['wrap'], # è®€å–è¨­å®š\n",
        "                        horizontal='left'\n",
        "                    )\n",
        "\n",
        "                    # æƒ…ç·’æ¬„ä½ç‰¹æ®Šç½®ä¸­èˆ‡ä¸Šè‰²\n",
        "                    if col_letter == 'G':\n",
        "                        cell.alignment = Alignment(horizontal='center', vertical='top')\n",
        "                        if cell.value == \"æ­£é¢\":\n",
        "                            cell.font = Font(color=\"006100\")\n",
        "                        elif cell.value == \"è² é¢\":\n",
        "                            cell.font = Font(color=\"9C0006\")\n",
        "\n",
        "    wb.save(file_path)\n",
        "    print(f\"ğŸ“Š å ±å‘Šå·²ç”¢å‡º: {file_path}\")\n",
        "\n",
        "# ç‚ºäº†ç›¸å®¹ä¸»ç¨‹å¼å‘¼å«ï¼Œä¿ç•™æ­¤å‡½å¼åç¨±\n",
        "def export_report(df, folder_path):\n",
        "    export_report_optimized(df, folder_path)"
      ],
      "metadata": {
        "id": "nTlcHVf8E34S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä¸»ç¨‹å¼ (Main Execution)"
      ],
      "metadata": {
        "id": "xNxFj8ZBE7jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def main():\n",
        "    # ==========================================\n",
        "    # [æ§åˆ¶å°] åŠŸèƒ½é–‹é—œ\n",
        "    # ==========================================\n",
        "    # å¯åœ¨æœ€ä¸Šæ–¹åˆå§‹åŒ–è¨­å®š\n",
        "    # ENABLE_KEYWORD_EXPANSION = True\n",
        "    # True: AI è‡ªå‹•è¯æƒ³æ“´å±• | False: åªæœåŸå§‹é—œéµå­—\n",
        "    # ==========================================\n",
        "\n",
        "    print(f\"ğŸš€ å°ˆæ¡ˆå•Ÿå‹• (AI æ“´å±•æ¨¡å¼: {'é–‹å•Ÿ' if ENABLE_KEYWORD_EXPANSION else 'é—œé–‰'})...\")\n",
        "\n",
        "    # 1. æº–å‚™çˆ¬èŸ²è¨­å®š (è™•ç†çµæ§‹ç›¸å®¹æ€§)\n",
        "    if ENABLE_KEYWORD_EXPANSION:\n",
        "        final_config = expand_keywords_structured(MONITOR_CONFIG)\n",
        "        print(f\"ğŸ“‹ å·²å»ºç«‹ AI æ“´å±•ç›£æ¸¬æ¸…å–®ã€‚\")\n",
        "    else:\n",
        "        print(\"ğŸ“‹ ä½¿ç”¨åŸå§‹ç›£æ¸¬æ¸…å–® (ç„¡æ“´å±•)ã€‚\")\n",
        "        final_config = {}\n",
        "        for topic, kws in MONITOR_CONFIG.items():\n",
        "            final_config[topic] = {}\n",
        "            for kw in kws:\n",
        "                # æ¯é—œéµå­—æ˜¯è‡ªå·±ï¼Œå­æœå°‹è©ä¹Ÿåªæœ‰è‡ªå·±\n",
        "                final_config[topic][kw] = [kw]\n",
        "\n",
        "    # 2. çˆ¬èŸ² (ä½¿ç”¨å …éŸŒç‰ˆ Module B)\n",
        "    df_raw = crawl_manager(final_config)\n",
        "\n",
        "    if df_raw.empty:\n",
        "        print(\"âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ï¼Œç¨‹å¼çµæŸã€‚\")\n",
        "        return\n",
        "\n",
        "    # 3. æ¸…æ´— (Module D)\n",
        "    df_clean = clean_and_filter_data(df_raw, LOOKBACK_DAYS)\n",
        "\n",
        "    # 4. AI æ‘˜è¦èˆ‡å¯©æŸ¥ (Module D)\n",
        "    df_analyzed = run_ai_pipeline(df_clean)\n",
        "\n",
        "    # 5. éæ¿¾èˆ‡æ¬„ä½æ•´ç†\n",
        "    # (A) éæ¿¾ä¸ç›¸é—œæ–°è\n",
        "    if 'is_relevant' in df_analyzed.columns:\n",
        "        print(\"\\nğŸ” æ­£åœ¨åŸ·è¡Œ AI é—œè¯æ€§éæ¿¾...\")\n",
        "\n",
        "        # åˆ†æµï¼šä¿ç•™ vs å‰”é™¤\n",
        "        df_final = df_analyzed[df_analyzed['is_relevant'] == True].copy()\n",
        "        dropped = df_analyzed[df_analyzed['is_relevant'] == False].copy()\n",
        "\n",
        "        removed_count = len(dropped)\n",
        "\n",
        "        if removed_count > 0:\n",
        "            print(f\"   âŒ å·²ç§»é™¤ {removed_count} å‰‡è¢« AI åˆ¤å®šç‚ºä¸ç›¸é—œçš„æ–°èã€‚\")\n",
        "\n",
        "            try:\n",
        "                # [é—œéµä¿®æ­£] ä½¿ç”¨ .sample() é€²è¡ŒçœŸéš¨æ©ŸæŠ½æ¨£\n",
        "                # min(5, len) ç¢ºä¿å¦‚æœå‰”é™¤å°‘æ–¼ 5 ç­†ä¸æœƒå ±éŒ¯\n",
        "                sample_size = min(10, len(dropped))\n",
        "\n",
        "                # random_state ç¢ºä¿æ¯æ¬¡åŸ·è¡Œçµæœä¸åŒ (æˆ–æ˜¯å›ºå®šä»¥ä¾¿å¾©ç¾ï¼Œé€™è£¡ä¸å›ºå®š)\n",
        "                samples = dropped.sample(n=sample_size)\n",
        "\n",
        "                print(f\"      ğŸ‘€ éš¨æ©ŸæŠ½æ¨£æª¢è¦– (å¾ {removed_count} ç­†ä¸­éš¨æ©ŸæŠ½ {sample_size} ç­†)ï¼š\")\n",
        "\n",
        "                for i, row in samples.iterrows():\n",
        "                    print(f\"      --------------------------------------------------\")\n",
        "                    # å˜—è©¦æŠ“å–å„ç¨®å¯èƒ½çš„é—œéµå­—æ¬„ä½\n",
        "                    kw = row.get('root_keyword', row.get('keyword', 'N/A'))\n",
        "\n",
        "                    print(f\"      [{i}] é—œéµå­—: {kw}\")\n",
        "                    print(f\"          æ¨™é¡Œ: {row['title']}\")\n",
        "                    # åŠ ä¸Šç´…è‰²é«˜äº®é¡¯ç¤ºç†ç”±\n",
        "                    print(f\"          ç†ç”±: \\033[35m{row.get('relevance_reason', 'N/A')}\\033[0m\")\n",
        "\n",
        "                print(f\"      --------------------------------------------------\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      (å°å‡ºç¯„ä¾‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e})\")\n",
        "        else:\n",
        "            print(\"   âœ… æ‰€æœ‰æ–°èçš†é€šéé—œè¯æ€§å¯©æŸ¥ã€‚\")\n",
        "    else:\n",
        "        df_final = df_analyzed\n",
        "\n",
        "    # (B) é¸æ“‡è¼¸å‡ºæ¬„ä½ (æ’é™¤ content)\n",
        "    cols_priority = [\n",
        "        'topic', 'root_keyword', 'source', 'date',\n",
        "        'sentiment', 'title', 'ai_summary', 'relevance_reason', 'link'\n",
        "    ]\n",
        "\n",
        "    final_cols = [c for c in cols_priority if c in df_final.columns]\n",
        "\n",
        "    # å¼·åˆ¶æ’é™¤ä¸éœ€è¦çš„æ¬„ä½\n",
        "    exclude_cols = ['content', 'id', 'is_relevant', 'temp_id', 'keyword']\n",
        "    remaining_cols = [c for c in df_final.columns if c not in final_cols and c not in exclude_cols]\n",
        "\n",
        "    # çµ„åˆæœ€çµ‚è¡¨æ ¼\n",
        "    export_df = df_final[final_cols + remaining_cols]\n",
        "\n",
        "    # 6. è¼¸å‡º Excel\n",
        "    export_report(export_df, OUTPUT_PATH)\n",
        "\n",
        "    print(\"\\nğŸ‰ğŸ‰ åŸ·è¡Œå®Œæˆï¼æ–°èè¿½è¹¤å ±å‘Šå·²ç”¢å‡ºã€‚\")\n",
        "    print(f\"ğŸ“‚ è«‹è‡³ Google Drive æŸ¥çœ‹: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "CIxU-i45E9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# åŸ·è¡Œå€å¡Š"
      ],
      "metadata": {
        "id": "u5zj7Jcao37M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dCs0ksolo7ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# å ±å‘Šæª¢æŸ¥"
      ],
      "metadata": {
        "id": "L5f2VqIvuz_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç’°å¢ƒè¨­å®š\n",
        "# ==========================================\n",
        "# å¦‚æœé‚„æ²’ä¸‹è¼‰å­—å‹ï¼Œé€™è£¡é˜²å‘†ä¸‹è¼‰ä¸€ä¸‹\n",
        "if not os.path.exists('TaipeiSansTCBeta-Regular.ttf'):\n",
        "    !wget -q -O TaipeiSansTCBeta-Regular.ttf \"https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\"\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "if os.path.exists('TaipeiSansTCBeta-Regular.ttf'):\n",
        "    matplotlib.font_manager.fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')\n",
        "    matplotlib.rc('font', family='Taipei Sans TC Beta')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æœå°‹æœ€æ–°çš„ Excel (è·¨æ—¥æœŸ)\n",
        "# ==========================================\n",
        "def find_latest_excel():\n",
        "    # è¨­å®šåŸºç¤è·¯å¾‘ (ä¸åŒ…å«æ—¥æœŸ)\n",
        "    base_path = \"/content/drive/My Drive/News_Crawler_Reports\"\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°åŸºç¤è·¯å¾‘ï¼š{base_path}\")\n",
        "        print(\"   è«‹ç¢ºèª Google Drive æ˜¯å¦å·²æ›è¼‰ï¼Ÿ\")\n",
        "        return None\n",
        "\n",
        "    print(f\"ğŸ” æ­£åœ¨æœå°‹ {base_path} åº•ä¸‹æ‰€æœ‰çš„ Excel æª”...\")\n",
        "\n",
        "    excel_files = []\n",
        "    # éè¿´æœå°‹æ‰€æœ‰å­è³‡æ–™å¤¾\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".xlsx\") and not file.startswith(\"~$\"): # æ’é™¤æš«å­˜æª”\n",
        "                full_path = os.path.join(root, file)\n",
        "                excel_files.append(full_path)\n",
        "\n",
        "    if not excel_files:\n",
        "        print(\"âŒ æ‰¾ä¸åˆ°ä»»ä½• Excel æª”æ¡ˆã€‚\")\n",
        "        return None\n",
        "\n",
        "    # ä¾ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„\n",
        "    latest_file = max(excel_files, key=os.path.getctime)\n",
        "    print(f\"âœ… æ‰¾åˆ°æœ€æ–°çš„æª”æ¡ˆï¼š{latest_file}\")\n",
        "    return latest_file\n",
        "\n",
        "# ==========================================\n",
        "# 3. å ±å‘Šæª¢æŸ¥ä¸»ç¨‹å¼\n",
        "# ==========================================\n",
        "def check_final_report(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{file_path}\")\n",
        "        return\n",
        "\n",
        "    # è®€å– Excel\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    print(f\"ğŸ“Š å ±å‘Šå¥æª¢ï¼š{file_path.split('/')[-1]}\")\n",
        "    print(f\"   ğŸ“Œ ç¸½è³‡æ–™ç­†æ•¸ï¼š{len(df)} å‰‡\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    cols_map = {\n",
        "        'ä¾†æº': 'source', 'ç›£æ¸¬ä¸»é¡Œ': 'topic', 'é—œéµå­—': 'root_keyword',\n",
        "        'ç™¼å¸ƒæ™‚é–“': 'date', 'æ–°èæ¨™é¡Œ': 'title',\n",
        "        'AI é‡é»æ‘˜è¦': 'ai_summary', 'AI è©•èª': 'relevance_reason'\n",
        "    }\n",
        "    df.rename(columns=cols_map, inplace=True)\n",
        "\n",
        "    # [1] ä¾†æºåˆ†ä½ˆ\n",
        "    print(\"\\n[1] ä¾†æºåˆ†ä½ˆ:\")\n",
        "    print(df['source'].value_counts())\n",
        "\n",
        "    # [2] æ™‚é–“åˆ†ä½ˆ\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    print(f\"\\n[2] æ™‚é–“ç¯„åœ:\")\n",
        "    print(f\"   ğŸ—“ï¸ Start: {df['date'].min()}\")\n",
        "    print(f\"   ğŸ—“ï¸ End:   {df['date'].max()}\")\n",
        "\n",
        "    # [3] AI æ‘˜è¦æŠ½æŸ¥ (ä¿®æ”¹è™•ï¼šå¢åŠ ç‚º 5 å‰‡)\n",
        "    sample_count = 5  # <--- é€™è£¡è¨­å®šè¦çœ‹å¹¾å‰‡\n",
        "    print(f\"\\n[3] AI æ‘˜è¦æŠ½æŸ¥ (éš¨æ©Ÿ {sample_count} å‰‡):\")\n",
        "\n",
        "    if len(df) > 0:\n",
        "        # ç¢ºä¿è³‡æ–™ä¸å¤ æ™‚ä¸æœƒå ±éŒ¯ (å–æœ€å°å€¼)\n",
        "        real_sample_count = min(sample_count, len(df))\n",
        "        samples = df.sample(n=real_sample_count)\n",
        "\n",
        "        for i, row in samples.iterrows():\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"ğŸ”¹ æ¨™é¡Œï¼š{row.get('title', 'N/A')}\")\n",
        "            print(f\"   ä¾†æºï¼š{row.get('source', 'N/A')} | {row.get('topic', 'N/A')}\")\n",
        "            # ç¨å¾®æ”¾å¯¬æ‘˜è¦é¡¯ç¤ºå­—æ•¸ï¼Œè®“ä½ çœ‹å¾—æ›´å®Œæ•´\n",
        "            print(f\"   ğŸ¤– æ‘˜è¦ï¼š{str(row.get('ai_summary', 'N/A'))}\")\n",
        "            print(f\"   âœ… ç†ç”±ï¼š\\033[92m{str(row.get('relevance_reason', 'N/A'))}\\033[0m\")\n",
        "\n",
        "    # ç¹ªåœ–\n",
        "    if len(df) > 0:\n",
        "        try:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "            s_counts = df['source'].value_counts()\n",
        "            colors = sns.color_palette('pastel')[0:len(s_counts)]\n",
        "            axes[0].pie(s_counts, labels=s_counts.index, autopct='%1.1f%%', startangle=140, colors=colors, textprops={'fontsize': 12})\n",
        "            axes[0].set_title(\"æ–°èä¾†æºä½”æ¯”\", fontsize=16)\n",
        "\n",
        "            t_counts = df['topic'].value_counts()\n",
        "            sns.barplot(x=t_counts.index, y=t_counts.values, ax=axes[1], palette=\"viridis\", hue=t_counts.index, legend=False)\n",
        "            axes[1].set_title(\"å„ä¸»é¡Œæ–°èè²é‡\", fontsize=16)\n",
        "            axes[1].tick_params(axis='x', rotation=45, labelsize=12)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâš ï¸ ç¹ªåœ–å¤±æ•—: {e}\")\n",
        "\n",
        "# === åŸ·è¡Œ ===\n",
        "target_file = find_latest_excel()\n",
        "if target_file:\n",
        "    check_final_report(target_file)"
      ],
      "metadata": {
        "id": "e4OqPGcHVmaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ç°¡æ˜“å„€éŒ¶æ¿"
      ],
      "metadata": {
        "id": "tL9KMrhtotzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio > /dev/null 2>&1\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç’°å¢ƒèˆ‡å­—å‹è¨­å®š\n",
        "# ==========================================\n",
        "font_filename = \"TaipeiSansTCBeta-Regular.ttf\"\n",
        "\n",
        "# æª¢æŸ¥å­—å‹æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ç„¡å‰‡ä¸‹è¼‰\n",
        "if not os.path.exists(font_filename):\n",
        "    !wget -q -O TaipeiSansTCBeta-Regular.ttf \"https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\"\n",
        "\n",
        "# è¨­å®šå…¨åŸŸå­—å‹\n",
        "try:\n",
        "    fm.fontManager.addfont(font_filename)\n",
        "    plt.rcParams['font.family'] = 'Taipei Sans TC Beta'\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    print(\"âœ… å­—å‹è¨­å®šå®Œæˆ (Taipei Sans TC Beta)\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ å­—å‹è¨­å®šå¤±æ•—: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. è³‡æ–™è¼‰å…¥é‚è¼¯ (è¨˜æ†¶é«” -> ç¡¬ç¢Ÿ)\n",
        "# ==========================================\n",
        "def load_data():\n",
        "    # å„ªå…ˆå˜—è©¦è®€å–è¨˜æ†¶é«”ä¸­çš„è®Šæ•¸\n",
        "    if 'export_df' in globals() and isinstance(export_df, pd.DataFrame) and not export_df.empty:\n",
        "        print(\"ğŸ“‚ è®€å–ä¾†æº: è¨˜æ†¶é«”è®Šæ•¸ (export_df)\")\n",
        "        return export_df.copy()\n",
        "\n",
        "    # è‹¥ç„¡ï¼Œå‰‡æœå°‹ç¡¬ç¢Ÿä¸­æœ€æ–°çš„ Excel\n",
        "    print(\"ğŸ“‚ è®€å–ä¾†æº: ç¡¬ç¢Ÿæª”æ¡ˆ (æœå°‹ä¸­...)\")\n",
        "    base_path = \"/content/drive/My Drive/News_Crawler_Reports\"\n",
        "    if not os.path.exists(base_path):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    excel_files = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".xlsx\") and not file.startswith(\"~$\"):\n",
        "                excel_files.append(os.path.join(root, file))\n",
        "\n",
        "    if excel_files:\n",
        "        latest_file = max(excel_files, key=os.path.getctime)\n",
        "        print(f\"   -> è¼‰å…¥æª”æ¡ˆ: {latest_file.split('/')[-1]}\")\n",
        "        return pd.read_excel(latest_file)\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# è¼‰å…¥è³‡æ–™\n",
        "df_dashboard = load_data()\n",
        "\n",
        "# ==========================================\n",
        "# 3. å„€è¡¨æ¿æ ¸å¿ƒé‚è¼¯\n",
        "# ==========================================\n",
        "def dashboard_analytics(topic_filter):\n",
        "    if df_dashboard.empty:\n",
        "        return \"âŒ ç„¡è³‡æ–™å¯é¡¯ç¤º\", None, None, []\n",
        "\n",
        "    # ç¯©é¸è³‡æ–™\n",
        "    df_view = df_dashboard.copy()\n",
        "    if topic_filter != \"å…¨éƒ¨\":\n",
        "        df_view = df_view[df_view['ç›£æ¸¬ä¸»é¡Œ'] == topic_filter]\n",
        "\n",
        "    total_count = len(df_view)\n",
        "    if total_count == 0:\n",
        "        return \"æŸ¥ç„¡ç¬¦åˆè³‡æ–™\", None, None, []\n",
        "\n",
        "    # --- åœ–è¡¨ A: ä¾†æºåˆ†ä½ˆ ---\n",
        "    fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "    source_counts = df_view['ä¾†æº'].value_counts()\n",
        "    # ä½¿ç”¨ pastel è‰²ç³»æ¯”è¼ƒæŸ”å’Œ\n",
        "    colors = sns.color_palette('pastel')[0:len(source_counts)]\n",
        "    ax1.pie(source_counts, labels=source_counts.index, autopct='%1.1f%%',\n",
        "            startangle=140, colors=colors, textprops={'fontsize': 10})\n",
        "    ax1.set_title(f\"æ–°èä¾†æºä½”æ¯”\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- åœ–è¡¨ B: æƒ…ç·’åˆ†ä½ˆ ---\n",
        "    fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "    if 'æƒ…ç·’' in df_view.columns:\n",
        "        sentiment_counts = df_view['æƒ…ç·’'].value_counts()\n",
        "        # æŒ‡å®šé¡è‰²ï¼šæ­£é¢ç¶ ã€è² é¢ç´…ã€ä¸­ç«‹ç°\n",
        "        color_map = {'æ­£é¢': '#2ca02c', 'è² é¢': '#d62728', 'ä¸­ç«‹': '#7f7f7f'}\n",
        "        sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, ax=ax2, palette=color_map)\n",
        "        ax2.set_title(f\"AI æƒ…ç·’åˆ†æ\", fontsize=14)\n",
        "        ax2.set_ylabel(\"ç¯‡æ•¸\")\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, \"ç„¡æƒ…ç·’è³‡æ–™\", ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- è¡¨æ ¼é è¦½ ---\n",
        "    # é¸å–é‡è¦æ¬„ä½å±•ç¤º\n",
        "    preview_cols = ['æ–°èæ¨™é¡Œ', 'ç™¼å¸ƒæ™‚é–“', 'AI é‡é»æ‘˜è¦', 'AI è©•èª']\n",
        "    # ç¢ºä¿æ¬„ä½å­˜åœ¨\n",
        "    valid_cols = [c for c in preview_cols if c in df_view.columns]\n",
        "    preview_data = df_view[valid_cols].head(10).values.tolist()\n",
        "\n",
        "    return f\"å…±æ‰¾åˆ° {total_count} å‰‡æ–°è\", fig1, fig2, preview_data\n",
        "\n",
        "# ==========================================\n",
        "# 4. å»ºç«‹ Gradio ä»‹é¢\n",
        "# ==========================================\n",
        "try:\n",
        "    # è£½ä½œä¸‹æ‹‰é¸å–®é¸é …\n",
        "    if not df_dashboard.empty and 'ç›£æ¸¬ä¸»é¡Œ' in df_dashboard.columns:\n",
        "        topics = [\"å…¨éƒ¨\"] + list(df_dashboard['ç›£æ¸¬ä¸»é¡Œ'].unique())\n",
        "    else:\n",
        "        topics = [\"å…¨éƒ¨\"]\n",
        "except:\n",
        "    topics = [\"å…¨éƒ¨\"]\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ğŸ“Š AI å•†æ¥­æƒ…å ±å„€éŒ¶æ¿ (Live Demo)\")\n",
        "    gr.Markdown(f\"è³‡æ–™æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')} | ä¾†æº: Python è‡ªå‹•åŒ–çˆ¬èŸ²ç³»çµ±\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            topic_dropdown = gr.Dropdown(choices=topics, value=\"å…¨éƒ¨\", label=\"ğŸ” é¸æ“‡ç›£æ¸¬ä¸»é¡Œ\")\n",
        "            result_text = gr.Textbox(label=\"çµ±è¨ˆæ•¸æ“š\", value=\"è«‹é¸æ“‡ä¸»é¡Œä»¥é–‹å§‹åˆ†æ\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            pass # ç©ºç™½ä½”ä½ç”¨\n",
        "\n",
        "    with gr.Row():\n",
        "        plot1 = gr.Plot(label=\"ä¾†æºåˆ†ä½ˆ\")\n",
        "        plot2 = gr.Plot(label=\"æƒ…ç·’è¶¨å‹¢\")\n",
        "\n",
        "    gr.Markdown(\"### ğŸ“‹ æœ€æ–°é«˜åƒ¹å€¼æƒ…å ± (å‰ 10 ç¯‡)\")\n",
        "    data_table = gr.Dataframe(\n",
        "        headers=[\"æ¨™é¡Œ\", \"æ™‚é–“\", \"æ‘˜è¦\", \"AI è©•èª\"],\n",
        "        datatype=[\"str\", \"str\", \"str\", \"str\"],\n",
        "        wrap=True,\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # è¨­å®šäº’å‹•é‚è¼¯ (ç•¶ä¸‹æ‹‰é¸å–®æ”¹è®Šæ™‚è§¸ç™¼)\n",
        "    topic_dropdown.change(\n",
        "        fn=dashboard_analytics,\n",
        "        inputs=topic_dropdown,\n",
        "        outputs=[result_text, plot1, plot2, data_table]\n",
        "    )\n",
        "\n",
        "    # å•Ÿå‹•æ™‚è‡ªå‹•åŸ·è¡Œä¸€æ¬¡\n",
        "    demo.load(\n",
        "        fn=dashboard_analytics,\n",
        "        inputs=topic_dropdown,\n",
        "        outputs=[result_text, plot1, plot2, data_table]\n",
        "    )\n",
        "\n",
        "# å•Ÿå‹• (share=True å¯ä»¥ç”¢ç”Ÿå…¬é–‹é€£çµåˆ†äº«çµ¦åˆ¥äººçœ‹ï¼Œé©åˆ Demo)\n",
        "print(\"ğŸš€ å„€éŒ¶æ¿å•Ÿå‹•ä¸­... è«‹é»æ“Šä¸‹æ–¹çš„ URL\")\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "30bU3fidozkF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}